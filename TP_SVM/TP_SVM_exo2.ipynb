{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c408f17d",
   "metadata": {},
   "source": [
    "# Les _support vector machines_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da84c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Les imports de base\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ea0bb9",
   "metadata": {},
   "source": [
    "## Historique des SVM\n",
    "\n",
    "Les Support Vector Machines (SVM) ont été introduits dans les années 1990 par Vladimir Vapnik et ses collaborateurs, dans le cadre du développement de la [théorie de Vapnik-Chervonenkis](https://fr.wikipedia.org/wiki/Th%C3%A9orie_de_Vapnik-Chervonenkis) (un nom compliqué pour parler d'apprentissage statistique...). Un SVM est un modèle d'apprentissage supervisé destiné à résoudre des problèmes de classification et de régression en trouvant un hyperplan optimal qui sépare les données en classes distinctes avec une marge maximale.\n",
    "\n",
    "Le concept de base des SVMs repose sur la minimisation de l'erreur de classification tout en maximisant la marge entre les classes. Ce modèle a été largement popularisé grâce à ses performances remarquables (pour l'époque) et à sa capacité à gérer des espaces de haute dimension, faisant des SVM un choix populaire pour une variété de tâches d'apprentissage automatique.\n",
    "\n",
    "Avec le temps, plusieurs algorithmes ont été développés pour résoudre les problèmes de SVM, y compris l'[algorithme SMO (_Sequential minimal optimization_)](https://cs229.stanford.edu/materials/smo.pdf) et les méthodes basées sur les points intérieurs. Ces développements ont permis d'améliorer l'efficacité et la scalabilité des SVM dans des applications pratiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cef3e5d",
   "metadata": {},
   "source": [
    "Les SVMs ont été développés à la base pour des problèmes de classification binaire (mais il est possible d'étendre leur fonctionnement aux problèmes multi-classes). Leur objectif est de trouver un hyperplan qui sépare les données en deux classes de manière optimale. L'idée clé est de maximiser la marge, c'est-à-dire la distance entre les échantillons les plus proches des deux classes et l'hyperplan séparateur. Les points les plus proches de l'hyperplan sont appelés **vecteurs supports**.\n",
    "\n",
    "## Formulation du problème primal avec marge dure\n",
    "\n",
    "Étant donné un problème de classification binaire $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ avec $\\mathbf{x}_i \\in \\mathbb{R}^p$ un échantillon (de dimension $p$) et $y_i \\in \\{-1,+1\\}$ sa classe associée.\n",
    "\n",
    "Si les données sont **linéairement séparables**, alors il existe un hyperplan $\\mathcal{H} = (\\mathbf{w},b)$ d'équation $\\mathbf{w}^T \\mathbf{x} + b = 0$ qui sépare parfaitement les données de la classe $+1$ de celles de la classe $-1$ avec $\\mathbf{w} \\in \\mathbb{R}^p$ un vecteur normal à l'hyperplan, et $b \\in \\mathbb{R}$ le biais (ordonnée à l'origine).\n",
    "\n",
    "L'existence d'un tel hyperplan :\n",
    "* se traduit mathématiquement par le fait que $\\mathbf{w}^T \\mathbf{x}_i + b \\geq 0$ si $y_i = +1$ et $\\mathbf{w}^T \\mathbf{x}_i + b \\leq 0$ si $y_i = -1$, ou, dit de manière plus compacte, \n",
    "\n",
    "$$ y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 0 \\ \\ \\forall i = 1,\\dots,n $$\n",
    "* implique en fait qu'il existe une infinité de tels hyperplan $\\Rightarrow$ il faut donc un critère supplémentaire pour choisir l'hyperplan optimal.\n",
    "\n",
    "Le critère supplémentaire de choix de l'hyperplan optimal dans le cas du SVM est la **maximisation** de la marge, autrement dit, la distance entre l'hyperplan et les échantillons les plus proches des deux côtés de l'hyperplan.\n",
    "On peut alors montrer (cf le cours) que :\n",
    "* la marge (normalisée) s'écrit $M_\\mathcal{H} = \\frac{2}{\\| \\mathbf{w} \\|}$\n",
    "* les contraintes de séparabilité linéaire se réécrivent $ y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 \\ \\ \\forall i = 1,\\dots,n $\n",
    "\n",
    "La maximisation de la marge est ramenée à une minimisation d'un terme quadratique (totalement équivalent), pour aboutir au final à la formulation suivante du problème primal $(P)$ du SVM à marge dure :\n",
    "\n",
    "$$\\begin{align}\n",
    "\\min_{(\\mathbf{w},b) \\in \\mathbb{R}^{p}\\times \\mathbb{R}} \\quad & \\frac{1}{2} \\| \\mathbf{w} \\|^2 \\qquad (P)\\\\\n",
    "\\text{tel que} \\quad & y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 \\ \\forall i = 1,\\dots,n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "La fonction objective étant quadratique et les contraintes étant linéaires, on est bien en présence d'un problème de programmation quadratique (QP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc66983",
   "metadata": {},
   "source": [
    "## Formulation du problème dual\n",
    "\n",
    "L'application des conditions KKT au problème primal permettent de formuler le problème dual $(D)$ associé au problème primal $(P)$ précédent pour le SVM, à savoir :\n",
    "\n",
    "$$\\begin{align}\n",
    "\\max_{\\boldsymbol \\alpha \\in \\mathbb{R}^{n}} \\quad & \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j \\qquad (D) \\\\\n",
    "\\text{tel que} \\quad & \\alpha_i \\geq 0 \\ \\ \\forall i = 1, \\dots, n \\\\\n",
    "& \\sum_{i=1}^n \\alpha_i y_i = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "où $\\boldsymbol \\alpha \\in \\mathbb{R}^n$ est la variable duale constituée des $n$ multiplicateurs de Lagrange associés aux $n$ contraintes d'inégalité du problème primal.\n",
    "\n",
    "Le problème dual est également un problème QP, de contraintes $\\alpha_i \\geq 0 \\ \\ \\forall i = 1,\\dots,n$ (admissibilité duale) et $\\sum_{i=1}^n \\alpha_i y_i = 0$ (relation découlant de la stationnarité du Lagrangien).\n",
    "Il peut être reformulé sous forme vectorielle comme : \n",
    "\n",
    "$$\\begin{align}\n",
    "\\max_{\\boldsymbol \\alpha \\in \\mathbb{R}^{n}} \\quad & \\mathbf{1}^T \\boldsymbol \\alpha - \\frac{1}{2} \\boldsymbol \\alpha^T \\mathbf{Q} \\boldsymbol \\alpha \\qquad (D)\\\\\n",
    "\\text{tel que} \\quad & \\alpha_i \\geq 0 \\ \\ \\forall i = 1, \\dots, n \\\\\n",
    "& \\mathbf{y}^T \\boldsymbol \\alpha = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "ou\n",
    "- $ \\mathbf{1} \\in \\mathbb{R}^n $ est un vecteur de 1.\n",
    "- $ \\mathbf{Q} \\in \\mathbb{R}^{n \\times n} $ est la matrice des produits scalaires des échantillons multipliés par le produit de leur classe $ Q_{ij} = y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j $.\n",
    "- $ \\mathbf{y} \\in \\mathbb{R}^n $ est le vecteur des labels, avec $ y_i \\in \\{-1, 1\\} $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54907abf",
   "metadata": {},
   "source": [
    "## Solution du problème primal\n",
    "\n",
    "Une fois la solution optimale $ \\boldsymbol \\alpha^\\star $ du problème dual trouvée, on peut en déduire la solution optimale du problème primal de la manière suivante :\n",
    "\n",
    "- **Vecteur normal à l'hyperplan optimal $ \\mathbf{w}^\\star $** : \n",
    "  $$\n",
    "  \\mathbf{w}^\\star = \\sum_{i=1}^{n} \\alpha_i^\\star y_i \\mathbf{x}_i\n",
    "  $$\n",
    "- **Biais de l'hyperplan optimal $ b^\\star $** : Le biais optimal peut être ensuite calculé à partir des vecteurs de support (par exemple à partir de n'importe quel $ \\mathbf{x}_i $ tel que $ \\alpha_i^\\star > 0 $) en utilisant l'équation :\n",
    "  $$\n",
    "  y_i \\big( (\\mathbf{w}^\\star)^T \\mathbf{x}_i + b^\\star)\\big) = 1\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe40424",
   "metadata": {},
   "source": [
    "## Génération d'un jeu de données linéairement séparable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270e6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea3c99b",
   "metadata": {},
   "source": [
    "Avant toute chose, générons un jeu de données de classification binaire **linéairement séparable**. Pour ce TP, on va se restreindre à des échantillons à deux dimensions $\\mathbf{x}_i \\in \\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération d'un jeu de données linéairement séparable\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, \n",
    "                           n_clusters_per_class=1, flip_y=0, class_sep=2.0, random_state=42)\n",
    "# Ajout d'un bruit léger aux données\n",
    "np.random.seed(seed=42)\n",
    "noise = np.random.normal(scale=0.4, size=X.shape)\n",
    "X += noise\n",
    "\n",
    "cls = np.unique(y)\n",
    "# Affichage des données\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[y==cls[0]][:, 0], X[y==cls[0]][:, 1], color='red', label='Classe %d'%cls[0], edgecolor='k')\n",
    "plt.scatter(X[y==cls[1]][:, 0], X[y==cls[1]][:, 1], color='blue', label='Classe %d'%cls[1], edgecolor='k')\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('Jeu de données linéairement séparable')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a89d26",
   "metadata": {},
   "source": [
    "Dans l'exemple précédent, la labellisation des classes a été faite de manière standard par `scikit-learn`, à savoir $y_i \\in \\{0,1\\}$ pour un problème binaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b5522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels des classes générées :\",np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93abc2df",
   "metadata": {},
   "source": [
    "En revanche, pour la formulation du problème du SVM, les deux classes doivent être labellisée par $y_i \\in \\{ -1, +1 \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e0860a",
   "metadata": {},
   "source": [
    "### 🛠️ 🚧 👷  À vous de jouer !\n",
    "\n",
    "Relabellisez les données précédentes (le vecteur `y`) pour vous placer dans le cas attendu pour la résolution du SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9dd19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ??? # FIXME ⚠️\n",
    "print(\"Labels des classes après relabellisation :\",np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e5230",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = np.unique(y)\n",
    "# Affichage des données\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[y==cls[0], 0], X[y==cls[0], 1], color='red', label='Classe %d'%cls[0], edgecolor='k')\n",
    "plt.scatter(X[y==cls[1], 0], X[y==cls[1], 1], color='blue', label='Classe %d'%cls[1], edgecolor='k')\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('Jeu de données linéairement séparable')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30df01a",
   "metadata": {},
   "source": [
    "## Résolution du SVM avec marge dure en utilisant `scipy.optimize.minimize`\n",
    "\n",
    "Maintenant qu'un jeu de données linéairement séparable a été généré et labellisé en accord avec le modèle du SVM, nous allons passer à la résolution de son problème dual en utilisant tout d'abord la fonction `minimize` de `scipy.optimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9418dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a02033b",
   "metadata": {},
   "source": [
    "### 🛠️ 🚧 👷  À vous de jouer !\n",
    "\n",
    "Résolvez le problème dual du SVM grâce à `minimize`, avec la démarche suivante :\n",
    "1. Le problème dual $(D)$ étant un problème de **maximisation**, et la fonction `minimize` étant (comme son nom l'indique), une routine permettant de **minimiser** une fonction objective, reformulez tout d'abord le problème dual pour l'écrire comme un problème de minimisation.\n",
    "2. Définissez explicitement la matrice $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$ de terme général $ Q_{ij} = y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j $.\n",
    "3. Définissez la fonction objective à minimiser selon le format attendu par `minimize`.\n",
    "4. Définissez les fonctions de contraintes du problème dual selon le format attendu par `minimize`.<br>\n",
    "<u>Note</u> : la contrainte $\\alpha_i \\geq 0$ peut être définie soit via une contrainte d'inégalité, soit grâce à l'argument `bounds`.\n",
    "5. Résolvez numériquement le problème dual grâce à `minimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db61f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition de la matrice Q de l'objective duale\n",
    "Q = ??? # # FIXME ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition de la fonction objective duale\n",
    "def objective_function(x):\n",
    "    ??? # FIXME ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb2673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition des contraintes\n",
    "def eq_constraint:\n",
    "    ??? # FIXME ⚠️\n",
    "\n",
    "def ineq_constraint:\n",
    "    ??? # FIXME ⚠️\n",
    "    \n",
    "constraints = ??? # FIXME ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc83b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sp = minimize(???) # FIXME ⚠️\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Variable duale optimale :\", result_sp.x)\n",
    "print(\"Valeur optimale de la fonction objective :\", result_sp.fun)\n",
    "print(\"Succès de l'optimisation :\", result_sp.success)\n",
    "print(\"Message :\", result_sp.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b631ed",
   "metadata": {},
   "source": [
    "### 🛠️ 🚧 👷  À vous de jouer !\n",
    "\n",
    "Une fois le problème dual résolu et le multiplicateur de Lagrange optimal $\\boldsymbol \\alpha^\\star$ obtenu, déterminez :\n",
    "1. Les vecteurs de support $\\mathbf{x}_s$ (tels que $ \\alpha_s^\\star > 0 $)\n",
    "2. Le vecteur normal $\\mathbf{w}^\\star$ de l'hyperplan optimal : $\\displaystyle \\mathbf{w}^\\star = \\sum_{i=1}^{n} \\alpha_i^\\star y_i \\mathbf{x}_i$\n",
    "3. Le biais $ b^\\star$ de l'hyperplan optimal $y_i \\big( (\\mathbf{w}^\\star)^T \\mathbf{x}_i + b^\\star)\\big) = 1$ calculé à partir d'un vecteur de support. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d50aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = result_sp.x\n",
    "\n",
    "# Calcul des vecteurs de support\n",
    "Xs_indices = ??? # FIXME ⚠️ indices (booléen) des vecteurs de support\n",
    "ys = ??? # FIXME ⚠️ labels des vecteurs de support\n",
    "Xs = ??? # FIXME ⚠️ coordonnées de vecteurs de support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f8cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des paramètres de l'hyperplan optimal\n",
    "w_sp = ??? # FIXME ⚠️ vecteur normal à l'hyperplan\n",
    "b_sp = ??? # FIXME ⚠️ biais de l'hyperplanred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ea5b02",
   "metadata": {},
   "source": [
    "### Affichage de l'hyperplan optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8526216",
   "metadata": {},
   "source": [
    "Pour finir, voici une fonction vous permettant d'afficher sur le scatterplot des données : \n",
    "- les vecteurs de supports `Xs` (et leur classe associée `ys`) que vous avez calculé à la question précédente.\n",
    "- l'hyperplan optimal du SVM de paramètres $\\mathbf{w}^\\star$ et $b^\\star$\n",
    "- les hyperplans passant par les vecteurs de support des deux classes (d'équation ${(\\mathbf{w}^\\star)}^T \\mathbf{x} + b^\\star = \\pm 1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e5212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm(X, y, Xs, ys, w_star, b_star):\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.grid(True)\n",
    "    # Classe -1\n",
    "    plt.scatter(X[y==-1,0], X[y==-1,1],\n",
    "                color='red',label='Classe -1',marker='o',edgecolor='k')\n",
    "    plt.scatter(Xs[ys==-1,0], Xs[ys==-1,1],\n",
    "                color='red',label='Vecteurs de support -1',marker='o',edgecolors='k',s=150)\n",
    "    # Classe +1\n",
    "    plt.scatter(X[y==1,0], X[y==1,1],color='blue', label='Classe +1',marker='o',edgecolor='k')\n",
    "    plt.scatter(Xs[ys==1,0], Xs[ys==1,1], \n",
    "                color='blue',label='Vecteurs de support +1',marker='o',edgecolors='k',s=150)\n",
    "      \n",
    "    xmin = X[:,0].min()-0.5\n",
    "    xmax = X[:,0].max()+0.5\n",
    "    # Hyperplan optimal\n",
    "    x_vals = np.linspace(xmin, xmax, 200)\n",
    "    y_vals = -(w_star[0]*x_vals+b_star)/w_star[1]\n",
    "    plt.plot(x_vals, y_vals, 'k-', label='Hyperplan optimal')\n",
    "    # Hyperplans passant par les vecteurs de support (w.x+b=±1)\n",
    "    y_vals_support1 = -(w_star[0]*x_vals+(b_star-1))/w_star[1]\n",
    "    y_vals_support2 = -(w_star[0]*x_vals+(b_star+1))/w_star[1]\n",
    "    plt.plot(x_vals, y_vals_support1, 'k--', label='Hyperplan +1')\n",
    "    plt.plot(x_vals, y_vals_support2, 'k-.', label='Hyperplan -1')\n",
    "    # Titre, labels et légende\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.title('Hyperplan optimal et vecteurs de support')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef3f2b8",
   "metadata": {},
   "source": [
    "### 🛠️ 🚧 👷  À vous de jouer !\n",
    "\n",
    "Affichez la solution que vous avez trouvé pour le SVM. Si celle-ci est correcte, la figure devrait faire sens...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45a0fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svm(X,y,Xs,ys,w_sp,b_sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42179f8d",
   "metadata": {},
   "source": [
    "## Résolution du SVM avec marge dure en utilisant `cvxopt`\n",
    "\n",
    "Vous devriez être convaincus que le problème dual du SVM est bien un programme quadratique. Il est donc maintenant temps de passer à sa résolution en vous servant du solveur `qp` de `cvxopt` (qui, rappel, ne fonctionne **que** pour les programmes quadratiques au contraire de `minimize`), dont le format standard pour le solveur est \n",
    "\n",
    "$$\\begin{aligned}\n",
    "& \\text{minimiser} & \\frac{1}{2} \\mathbf{x}^T \\mathbf{P} \\mathbf{x} + \\mathbf{q}^T \\mathbf{x} \\\\\n",
    "& \\text{sous les contraintes} & \\mathbf{G}\\mathbf{x} \\leq \\mathbf{h} \\\\\n",
    "& & \\mathbf{A}\\mathbf{x} = \\mathbf{b}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Pour résoudre le problème dual du SVM avec `qp`, il vous faudra donc \"juste\" spécifier (au format `matrix` de `cvxopt`) les définitions de ces matrices $\\mathbf{P}$, $\\mathbf{G}$, $\\mathbf{A}$ et des vecteurs $\\mathbf{q}$, $\\mathbf{h}$, $\\mathbf{b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beab069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxopt\n",
    "from cvxopt import printing\n",
    "cvxopt.matrix_repr = printing.matrix_str_default\n",
    "from cvxopt import matrix\n",
    "from cvxopt.solvers import qp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e6e0fd",
   "metadata": {},
   "source": [
    "### 🛠️ 🚧 👷  À vous de jouer !\n",
    "\n",
    "Résolvez le problème dual du SVM grâce au solveur `qp` de `cvxopt`, avec la démarche suivante :\n",
    "1. Tout comme pour `minimize`, le solveur `qp` de `cvxopt` permet de **minimiser** une fonction objective donnée. La reformulation du problème dual que vous avez (normalement) fait à la question précédente avec `minimize` reste donc valable ici...\n",
    "2. Identifiez puis définissez les matrices $\\mathbf{P}$, $\\mathbf{G}$, $\\mathbf{A}$ et les vecteurs $\\mathbf{q}$, $\\mathbf{h}$, $\\mathbf{b}$ attendus par `qp`.\n",
    "5. Résolvez numériquement le problème dual grâce à `qp`.\n",
    "\n",
    "⚠️ Il est possible que vous obteniez une erreur du type \n",
    "```python\n",
    "TypeError: 'A' must be a 'd' matrix with 100 columns\n",
    "```\n",
    "\n",
    "`cvxopt` utilise plusieurs formats de stockage des matrices, en fonction du type des entrées (entiers, flottants, complexes). Si vous initialisez un `matrix` à partir d'un `array` entier, vous obtiendrez un encodage entier (sans surprise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef840afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([2,1])\n",
    "matrix(A).typecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fbb13a",
   "metadata": {},
   "source": [
    "L'erreur précédente vous indique que `qp` attend que le paramètre (matrice ou vecteur) qui lui est passés en arguments soit de type `'d'` (flottant). Pour cela, il suffit juste de multiplier l'`array` qui sert à instancier la variable `matrix` par `1.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a552d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([2,1])\n",
    "matrix(1.*A).typecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28adae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition des matrices/vecteurs (au format matrix) pour le problème dual\n",
    "P = ??? # FIXME ⚠️\n",
    "q = ??? # FIXME ⚠️\n",
    "G = ??? # FIXME ⚠️\n",
    "h = ??? # FIXME ⚠️\n",
    "A = ??? # FIXME ⚠️\n",
    "b = ??? # FIXME ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b950bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# résolution du problème dual\n",
    "result_cvxopt = qp(???) # FIXME ⚠️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99724e3b",
   "metadata": {},
   "source": [
    "### 🛠️ 🚧 👷  À vous de jouer !\n",
    "\n",
    "Idem que pour la question précédente avec `minimize`, déterminez maintenant grâce au multiplicateur de Lagrange optimal $\\boldsymbol \\alpha^\\star$ obtenu avec `qp` :\n",
    "1. Les vecteurs de support $\\mathbf{x}_s$ (tels que $ \\alpha_s^\\star > 0 $)\n",
    "2. Le vecteur normal $\\mathbf{w}^\\star$ de l'hyperplan optimal : $\\displaystyle \\mathbf{w}^\\star = \\sum_{i=1}^{n} \\alpha_i^\\star y_i \\mathbf{x}_i$\n",
    "3. Le biais $ b^\\star$ de l'hyperplan optimal $y_i \\big( (\\mathbf{w}^\\star)^T \\mathbf{x}_i + b^\\star)\\big) = 1$ calculé à partir d'un vecteur de support. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd93154",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.array(result_cvxopt['x']).flatten()\n",
    "\n",
    "# Calcul des vecteurs de support\n",
    "Xs_indices = ??? # FIXME ⚠️ indices (booléen) des vecteurs de support\n",
    "ys = ??? # FIXME ⚠️ labels des vecteurs de support\n",
    "Xs = ??? # FIXME ⚠️ coordonnées de vecteurs de support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc4432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des paramètres de l'hyperplan optimal\n",
    "w_cvxopt = ??? # FIXME ⚠️ vecteur normal à l'hyperplan\n",
    "b_cvxopt = ??? # FIXME ⚠️ biais de l'hyperplan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea2a89",
   "metadata": {},
   "source": [
    "### 🛠️ 🚧 👷  À vous de jouer !\n",
    "\n",
    "Affichez la solution que vous avez trouvé pour le SVM avec `qp`. Si celle-ci est correcte (et sous réserve que celle que vous avez obtenu avec `minimize` l'était aussi), vous devriez donc obtenir la même chose..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1bab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svm(X,y,Xs,ys,w_cvxopt,b_cvxopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7fd8e0",
   "metadata": {},
   "source": [
    "### 🛠️ 🚧 👷  À vous de jouer !\n",
    "\n",
    "Sous réserve que vous avez trouvé les bonnes solutions avec `minimize` et `qp`, l'affichage du scatterplot avec les vecteurs de support et hyperplans optimaux doit normalement être le même dans les deux cas de figure (ouf).\n",
    "\n",
    "Vérifiez donc que les paramètres $\\mathbf{w}^\\star$ et $b^\\star$ concordent bien numériquement pour les deux méthodes : \n",
    "- est-ce vraiment le cas ?\n",
    "- que pouvez vous en conclure ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca4b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_sp ??? w_cvxopt # FIXME ⚠️ est-ce que les vecteurs normaux aux hyperplans concordent ?\n",
    "b_sp ??? b_cvxopt # FIXME ⚠️ est-ce que les biais des hyperplans concodent ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a70ed",
   "metadata": {},
   "source": [
    "On peut donc en conclude que ⚠️ FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b757c1",
   "metadata": {},
   "source": [
    "## Bravo ! 👏🍻\n",
    "\n",
    "Vous en avez terminé avec la résolution du SVM à marge dure. En route maintenant vers les [SVMs à marge douce](TP_SVM_exo3.ipynb) !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
