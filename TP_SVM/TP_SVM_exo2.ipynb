{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c408f17d",
   "metadata": {},
   "source": [
    "# Les _support vector machines_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da84c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Les imports de base\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ea0bb9",
   "metadata": {},
   "source": [
    "## Historique des SVM\n",
    "\n",
    "Les Support Vector Machines (SVM) ont √©t√© introduits dans les ann√©es 1990 par Vladimir Vapnik et ses collaborateurs, dans le cadre du d√©veloppement de la [th√©orie de Vapnik-Chervonenkis](https://fr.wikipedia.org/wiki/Th%C3%A9orie_de_Vapnik-Chervonenkis) (un nom compliqu√© pour parler d'apprentissage statistique...). Un SVM est un mod√®le d'apprentissage supervis√© destin√© √† r√©soudre des probl√®mes de classification et de r√©gression en trouvant un hyperplan optimal qui s√©pare les donn√©es en classes distinctes avec une marge maximale.\n",
    "\n",
    "Le concept de base des SVMs repose sur la minimisation de l'erreur de classification tout en maximisant la marge entre les classes. Ce mod√®le a √©t√© largement popularis√© gr√¢ce √† ses performances remarquables (pour l'√©poque) et √† sa capacit√© √† g√©rer des espaces de haute dimension, faisant des SVM un choix populaire pour une vari√©t√© de t√¢ches d'apprentissage automatique.\n",
    "\n",
    "Avec le temps, plusieurs algorithmes ont √©t√© d√©velopp√©s pour r√©soudre les probl√®mes de SVM, y compris l'[algorithme SMO (_Sequential minimal optimization_)](https://cs229.stanford.edu/materials/smo.pdf) et les m√©thodes bas√©es sur les points int√©rieurs. Ces d√©veloppements ont permis d'am√©liorer l'efficacit√© et la scalabilit√© des SVM dans des applications pratiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cef3e5d",
   "metadata": {},
   "source": [
    "Les SVMs ont √©t√© d√©velopp√©s √† la base pour des probl√®mes de classification binaire (mais il est possible d'√©tendre leur fonctionnement aux probl√®mes multi-classes). Leur objectif est de trouver un hyperplan qui s√©pare les donn√©es en deux classes de mani√®re optimale. L'id√©e cl√© est de maximiser la marge, c'est-√†-dire la distance entre les √©chantillons les plus proches des deux classes et l'hyperplan s√©parateur. Les points les plus proches de l'hyperplan sont appel√©s **vecteurs supports**.\n",
    "\n",
    "## Formulation du probl√®me primal avec marge dure\n",
    "\n",
    "√âtant donn√© un probl√®me de classification binaire $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ avec $\\mathbf{x}_i \\in \\mathbb{R}^p$ un √©chantillon (de dimension $p$) et $y_i \\in \\{-1,+1\\}$ sa classe associ√©e.\n",
    "\n",
    "Si les donn√©es sont **lin√©airement s√©parables**, alors il existe un hyperplan $\\mathcal{H} = (\\mathbf{w},b)$ d'√©quation $\\mathbf{w}^T \\mathbf{x} + b = 0$ qui s√©pare parfaitement les donn√©es de la classe $+1$ de celles de la classe $-1$ avec $\\mathbf{w} \\in \\mathbb{R}^p$ un vecteur normal √† l'hyperplan, et $b \\in \\mathbb{R}$ le biais (ordonn√©e √† l'origine).\n",
    "\n",
    "L'existence d'un tel hyperplan :\n",
    "* se traduit math√©matiquement par le fait que $\\mathbf{w}^T \\mathbf{x}_i + b \\geq 0$ si $y_i = +1$ et $\\mathbf{w}^T \\mathbf{x}_i + b \\leq 0$ si $y_i = -1$, ou, dit de mani√®re plus compacte, \n",
    "\n",
    "$$ y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 0 \\ \\ \\forall i = 1,\\dots,n $$\n",
    "* implique en fait qu'il existe une infinit√© de tels hyperplan $\\Rightarrow$ il faut donc un crit√®re suppl√©mentaire pour choisir l'hyperplan optimal.\n",
    "\n",
    "Le crit√®re suppl√©mentaire de choix de l'hyperplan optimal dans le cas du SVM est la **maximisation** de la marge, autrement dit, la distance entre l'hyperplan et les √©chantillons les plus proches des deux c√¥t√©s de l'hyperplan.\n",
    "On peut alors montrer (cf le cours) que :\n",
    "* la marge (normalis√©e) s'√©crit $M_\\mathcal{H} = \\frac{2}{\\| \\mathbf{w} \\|}$\n",
    "* les contraintes de s√©parabilit√© lin√©aire se r√©√©crivent $ y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 \\ \\ \\forall i = 1,\\dots,n $\n",
    "\n",
    "La maximisation de la marge est ramen√©e √† une minimisation d'un terme quadratique (totalement √©quivalent), pour aboutir au final √† la formulation suivante du probl√®me primal $(P)$ du SVM √† marge dure :\n",
    "\n",
    "$$\\begin{align}\n",
    "\\min_{(\\mathbf{w},b) \\in \\mathbb{R}^{p}\\times \\mathbb{R}} \\quad & \\frac{1}{2} \\| \\mathbf{w} \\|^2 \\qquad (P)\\\\\n",
    "\\text{tel que} \\quad & y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 \\ \\forall i = 1,\\dots,n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "La fonction objective √©tant quadratique et les contraintes √©tant lin√©aires, on est bien en pr√©sence d'un probl√®me de programmation quadratique (QP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc66983",
   "metadata": {},
   "source": [
    "## Formulation du probl√®me dual\n",
    "\n",
    "L'application des conditions KKT au probl√®me primal permettent de formuler le probl√®me dual $(D)$ associ√© au probl√®me primal $(P)$ pr√©c√©dent pour le SVM, √† savoir :\n",
    "\n",
    "$$\\begin{align}\n",
    "\\max_{\\boldsymbol \\alpha \\in \\mathbb{R}^{n}} \\quad & \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j \\qquad (D) \\\\\n",
    "\\text{tel que} \\quad & \\alpha_i \\geq 0 \\ \\ \\forall i = 1, \\dots, n \\\\\n",
    "& \\sum_{i=1}^n \\alpha_i y_i = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "o√π $\\boldsymbol \\alpha \\in \\mathbb{R}^n$ est la variable duale constitu√©e des $n$ multiplicateurs de Lagrange associ√©s aux $n$ contraintes d'in√©galit√© du probl√®me primal.\n",
    "\n",
    "Le probl√®me dual est √©galement un probl√®me QP, de contraintes $\\alpha_i \\geq 0 \\ \\¬†\\forall i = 1,\\dots,n$ (admissibilit√© duale) et $\\sum_{i=1}^n \\alpha_i y_i = 0$ (relation d√©coulant de la stationnarit√© du Lagrangien).\n",
    "Il peut √™tre reformul√© sous forme vectorielle comme : \n",
    "\n",
    "$$\\begin{align}\n",
    "\\max_{\\boldsymbol \\alpha \\in \\mathbb{R}^{n}} \\quad & \\mathbf{1}^T \\boldsymbol \\alpha - \\frac{1}{2} \\boldsymbol \\alpha^T \\mathbf{Q} \\boldsymbol \\alpha \\qquad (D)\\\\\n",
    "\\text{tel que} \\quad & \\alpha_i \\geq 0 \\ \\ \\forall i = 1, \\dots, n \\\\\n",
    "& \\mathbf{y}^T \\boldsymbol \\alpha = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "ou\n",
    "- $ \\mathbf{1} \\in \\mathbb{R}^n $ est un vecteur de 1.\n",
    "- $ \\mathbf{Q} \\in \\mathbb{R}^{n \\times n} $ est la matrice des produits scalaires des √©chantillons multipli√©s par le produit de leur classe $ Q_{ij} = y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j $.\n",
    "- $ \\mathbf{y} \\in \\mathbb{R}^n $ est le vecteur des labels, avec $ y_i \\in \\{-1, 1\\} $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54907abf",
   "metadata": {},
   "source": [
    "## Solution du probl√®me primal\n",
    "\n",
    "Une fois la solution optimale $ \\boldsymbol \\alpha^\\star $ du probl√®me dual trouv√©e, on peut en d√©duire la solution optimale du probl√®me primal de la mani√®re suivante :\n",
    "\n",
    "- **Vecteur normal √† l'hyperplan optimal $ \\mathbf{w}^\\star $** : \n",
    "  $$\n",
    "  \\mathbf{w}^\\star = \\sum_{i=1}^{n} \\alpha_i^\\star y_i \\mathbf{x}_i\n",
    "  $$\n",
    "- **Biais de l'hyperplan optimal $ b^\\star $** : Le biais optimal peut √™tre ensuite calcul√© √† partir des vecteurs de support (par exemple √† partir de n'importe quel $ \\mathbf{x}_i $ tel que $ \\alpha_i^\\star > 0 $) en utilisant l'√©quation :\n",
    "  $$\n",
    "  y_i \\big( (\\mathbf{w}^\\star)^T \\mathbf{x}_i + b^\\star)\\big) = 1\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe40424",
   "metadata": {},
   "source": [
    "## G√©n√©ration d'un jeu de donn√©es lin√©airement s√©parable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270e6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea3c99b",
   "metadata": {},
   "source": [
    "Avant toute chose, g√©n√©rons un jeu de donn√©es de classification binaire **lin√©airement s√©parable**. Pour ce TP, on va se restreindre √† des √©chantillons √† deux dimensions $\\mathbf{x}_i \\in \\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©ration d'un jeu de donn√©es lin√©airement s√©parable\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, \n",
    "                           n_clusters_per_class=1, flip_y=0, class_sep=2.0, random_state=42)\n",
    "# Ajout d'un bruit l√©ger aux donn√©es\n",
    "np.random.seed(seed=42)\n",
    "noise = np.random.normal(scale=0.4, size=X.shape)\n",
    "X += noise\n",
    "\n",
    "cls = np.unique(y)\n",
    "# Affichage des donn√©es\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[y==cls[0]][:, 0], X[y==cls[0]][:, 1], color='red', label='Classe %d'%cls[0], edgecolor='k')\n",
    "plt.scatter(X[y==cls[1]][:, 0], X[y==cls[1]][:, 1], color='blue', label='Classe %d'%cls[1], edgecolor='k')\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('Jeu de donn√©es lin√©airement s√©parable')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a89d26",
   "metadata": {},
   "source": [
    "Dans l'exemple pr√©c√©dent, la labellisation des classes a √©t√© faite de mani√®re standard par `scikit-learn`, √† savoir $y_i \\in \\{0,1\\}$ pour un probl√®me binaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b5522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels des classes g√©n√©r√©es :\",np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93abc2df",
   "metadata": {},
   "source": [
    "En revanche, pour la formulation du probl√®me du SVM, les deux classes doivent √™tre labellis√©e par $y_i \\in \\{ -1, +1 \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e0860a",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è üöß üë∑  √Ä vous de jouer !\n",
    "\n",
    "Relabellisez les donn√©es pr√©c√©dentes (le vecteur `y`) pour vous placer dans le cas attendu pour la r√©solution du SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9dd19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ??? # FIXME ‚ö†Ô∏è\n",
    "print(\"Labels des classes apr√®s relabellisation :\",np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e5230",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = np.unique(y)\n",
    "# Affichage des donn√©es\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[y==cls[0], 0], X[y==cls[0], 1], color='red', label='Classe %d'%cls[0], edgecolor='k')\n",
    "plt.scatter(X[y==cls[1], 0], X[y==cls[1], 1], color='blue', label='Classe %d'%cls[1], edgecolor='k')\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('Jeu de donn√©es lin√©airement s√©parable')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30df01a",
   "metadata": {},
   "source": [
    "## R√©solution du SVM avec marge dure en utilisant `scipy.optimize.minimize`\n",
    "\n",
    "Maintenant qu'un jeu de donn√©es lin√©airement s√©parable a √©t√© g√©n√©r√© et labellis√© en accord avec le mod√®le du SVM, nous allons passer √† la r√©solution de son probl√®me dual en utilisant tout d'abord la fonction `minimize` de `scipy.optimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9418dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a02033b",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è üöß üë∑  √Ä vous de jouer !\n",
    "\n",
    "R√©solvez le probl√®me dual du SVM gr√¢ce √† `minimize`, avec la d√©marche suivante :\n",
    "1. Le probl√®me dual $(D)$ √©tant un probl√®me de **maximisation**, et la fonction `minimize` √©tant (comme son nom l'indique), une routine permettant de **minimiser** une fonction objective, reformulez tout d'abord le probl√®me dual pour l'√©crire comme un probl√®me de minimisation.\n",
    "2. D√©finissez explicitement la matrice $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$ de terme g√©n√©ral $ Q_{ij} = y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j $.\n",
    "3. D√©finissez la fonction objective √† minimiser selon le format attendu par `minimize`.\n",
    "4. D√©finissez les fonctions de contraintes du probl√®me dual selon le format attendu par `minimize`.<br>\n",
    "<u>Note</u> : la contrainte $\\alpha_i \\geq 0$ peut √™tre d√©finie soit via une contrainte d'in√©galit√©, soit gr√¢ce √† l'argument `bounds`.\n",
    "5. R√©solvez num√©riquement le probl√®me dual gr√¢ce √† `minimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db61f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d√©finition de la matrice Q de l'objective duale\n",
    "Q = ??? # # FIXME ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d√©finition de la fonction objective duale\n",
    "def objective_function(x):\n",
    "    ??? # FIXME ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb2673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition des contraintes\n",
    "def eq_constraint:\n",
    "    ??? # FIXME ‚ö†Ô∏è\n",
    "\n",
    "def ineq_constraint:\n",
    "    ??? # FIXME ‚ö†Ô∏è\n",
    "    \n",
    "constraints = ??? # FIXME ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc83b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sp = minimize(???) # FIXME ‚ö†Ô∏è\n",
    "\n",
    "# Affichage des r√©sultats\n",
    "print(\"Variable duale optimale :\", result_sp.x)\n",
    "print(\"Valeur optimale de la fonction objective :\", result_sp.fun)\n",
    "print(\"Succ√®s de l'optimisation :\", result_sp.success)\n",
    "print(\"Message :\", result_sp.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b631ed",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è üöß üë∑  √Ä vous de jouer !\n",
    "\n",
    "Une fois le probl√®me dual r√©solu et le multiplicateur de Lagrange optimal $\\boldsymbol \\alpha^\\star$ obtenu, d√©terminez :\n",
    "1. Les vecteurs de support $\\mathbf{x}_s$ (tels que $ \\alpha_s^\\star > 0 $)\n",
    "2. Le vecteur normal $\\mathbf{w}^\\star$ de l'hyperplan optimal : $\\displaystyle \\mathbf{w}^\\star = \\sum_{i=1}^{n} \\alpha_i^\\star y_i \\mathbf{x}_i$\n",
    "3. Le biais $ b^\\star$ de l'hyperplan optimal $y_i \\big( (\\mathbf{w}^\\star)^T \\mathbf{x}_i + b^\\star)\\big) = 1$ calcul√© √† partir d'un vecteur de support. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d50aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = result_sp.x\n",
    "\n",
    "# Calcul des vecteurs de support\n",
    "Xs_indices = ??? # FIXME ‚ö†Ô∏è indices (bool√©en) des vecteurs de support\n",
    "ys = ??? # FIXME ‚ö†Ô∏è labels des vecteurs de support\n",
    "Xs = ??? # FIXME ‚ö†Ô∏è coordonn√©es de vecteurs de support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f8cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des param√®tres de l'hyperplan optimal\n",
    "w_sp = ??? # FIXME ‚ö†Ô∏è vecteur normal √† l'hyperplan\n",
    "b_sp = ??? # FIXME ‚ö†Ô∏è biais de l'hyperplanred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ea5b02",
   "metadata": {},
   "source": [
    "### Affichage de l'hyperplan optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8526216",
   "metadata": {},
   "source": [
    "Pour finir, voici une fonction vous permettant d'afficher sur le scatterplot des donn√©es : \n",
    "- les vecteurs de supports `Xs` (et leur classe associ√©e `ys`) que vous avez calcul√© √† la question pr√©c√©dente.\n",
    "- l'hyperplan optimal du SVM de param√®tres $\\mathbf{w}^\\star$ et $b^\\star$\n",
    "- les hyperplans passant par les vecteurs de support des deux classes (d'√©quation ${(\\mathbf{w}^\\star)}^T \\mathbf{x} + b^\\star = \\pm 1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e5212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm(X, y, Xs, ys, w_star, b_star):\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.grid(True)\n",
    "    # Classe -1\n",
    "    plt.scatter(X[y==-1,0], X[y==-1,1],\n",
    "                color='red',label='Classe -1',marker='o',edgecolor='k')\n",
    "    plt.scatter(Xs[ys==-1,0], Xs[ys==-1,1],\n",
    "                color='red',label='Vecteurs de support -1',marker='o',edgecolors='k',s=150)\n",
    "    # Classe +1\n",
    "    plt.scatter(X[y==1,0], X[y==1,1],color='blue', label='Classe +1',marker='o',edgecolor='k')\n",
    "    plt.scatter(Xs[ys==1,0], Xs[ys==1,1], \n",
    "                color='blue',label='Vecteurs de support +1',marker='o',edgecolors='k',s=150)\n",
    "      \n",
    "    xmin = X[:,0].min()-0.5\n",
    "    xmax = X[:,0].max()+0.5\n",
    "    # Hyperplan optimal\n",
    "    x_vals = np.linspace(xmin, xmax, 200)\n",
    "    y_vals = -(w_star[0]*x_vals+b_star)/w_star[1]\n",
    "    plt.plot(x_vals, y_vals, 'k-', label='Hyperplan optimal')\n",
    "    # Hyperplans passant par les vecteurs de support (w.x+b=¬±1)\n",
    "    y_vals_support1 = -(w_star[0]*x_vals+(b_star-1))/w_star[1]\n",
    "    y_vals_support2 = -(w_star[0]*x_vals+(b_star+1))/w_star[1]\n",
    "    plt.plot(x_vals, y_vals_support1, 'k--', label='Hyperplan +1')\n",
    "    plt.plot(x_vals, y_vals_support2, 'k-.', label='Hyperplan -1')\n",
    "    # Titre, labels et l√©gende\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.title('Hyperplan optimal et vecteurs de support')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef3f2b8",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è üöß üë∑  √Ä vous de jouer !\n",
    "\n",
    "Affichez la solution que vous avez trouv√© pour le SVM. Si celle-ci est correcte, la figure devrait faire sens...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45a0fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svm(X,y,Xs,ys,w_sp,b_sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42179f8d",
   "metadata": {},
   "source": [
    "## R√©solution du SVM avec marge dure en utilisant `cvxopt`\n",
    "\n",
    "Vous devriez √™tre convaincus que le probl√®me dual du SVM est bien un programme quadratique. Il est donc maintenant temps de passer √† sa r√©solution en vous servant du solveur `qp` de `cvxopt` (qui, rappel, ne fonctionne **que** pour les programmes quadratiques au contraire de `minimize`), dont le format standard pour le solveur est \n",
    "\n",
    "$$\\begin{aligned}\n",
    "& \\text{minimiser} & \\frac{1}{2} \\mathbf{x}^T \\mathbf{P} \\mathbf{x} + \\mathbf{q}^T \\mathbf{x} \\\\\n",
    "& \\text{sous les contraintes} & \\mathbf{G}\\mathbf{x} \\leq \\mathbf{h} \\\\\n",
    "& & \\mathbf{A}\\mathbf{x} = \\mathbf{b}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Pour r√©soudre le probl√®me dual du SVM avec `qp`, il vous faudra donc \"juste\" sp√©cifier (au format `matrix` de `cvxopt`) les d√©finitions de ces matrices $\\mathbf{P}$, $\\mathbf{G}$, $\\mathbf{A}$ et des vecteurs $\\mathbf{q}$, $\\mathbf{h}$, $\\mathbf{b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beab069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxopt\n",
    "from cvxopt import printing\n",
    "cvxopt.matrix_repr = printing.matrix_str_default\n",
    "from cvxopt import matrix\n",
    "from cvxopt.solvers import qp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e6e0fd",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è üöß üë∑  √Ä vous de jouer !\n",
    "\n",
    "R√©solvez le probl√®me dual du SVM gr√¢ce au solveur `qp` de `cvxopt`, avec la d√©marche suivante :\n",
    "1. Tout comme pour `minimize`, le solveur `qp` de `cvxopt` permet de **minimiser** une fonction objective donn√©e. La reformulation du probl√®me dual que vous avez (normalement) fait √† la question pr√©c√©dente avec `minimize` reste donc valable ici...\n",
    "2. Identifiez puis d√©finissez les matrices $\\mathbf{P}$, $\\mathbf{G}$, $\\mathbf{A}$ et les vecteurs $\\mathbf{q}$, $\\mathbf{h}$, $\\mathbf{b}$ attendus par `qp`.\n",
    "5. R√©solvez num√©riquement le probl√®me dual gr√¢ce √† `qp`.\n",
    "\n",
    "‚ö†Ô∏è Il est possible que vous obteniez une erreur du type \n",
    "```python\n",
    "TypeError: 'A' must be a 'd' matrix with 100 columns\n",
    "```\n",
    "\n",
    "`cvxopt` utilise plusieurs formats de stockage des matrices, en fonction du type des entr√©es (entiers, flottants, complexes). Si vous initialisez un `matrix` √† partir d'un `array` entier, vous obtiendrez un encodage entier (sans surprise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef840afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([2,1])\n",
    "matrix(A).typecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fbb13a",
   "metadata": {},
   "source": [
    "L'erreur pr√©c√©dente vous indique que `qp` attend que le param√®tre (matrice ou vecteur) qui lui est pass√©s en arguments soit de type `'d'` (flottant). Pour cela, il suffit juste de multiplier l'`array` qui sert √† instancier la variable `matrix` par `1.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a552d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([2,1])\n",
    "matrix(1.*A).typecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28adae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d√©finition des matrices/vecteurs (au format matrix) pour le probl√®me dual\n",
    "P = ??? # FIXME ‚ö†Ô∏è\n",
    "q = ??? # FIXME ‚ö†Ô∏è\n",
    "G = ??? # FIXME ‚ö†Ô∏è\n",
    "h = ??? # FIXME ‚ö†Ô∏è\n",
    "A = ??? # FIXME ‚ö†Ô∏è\n",
    "b = ??? # FIXME ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b950bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r√©solution du probl√®me dual\n",
    "result_cvxopt = qp(???) # FIXME ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99724e3b",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è üöß üë∑  √Ä vous de jouer !\n",
    "\n",
    "Idem que pour la question pr√©c√©dente avec `minimize`, d√©terminez maintenant gr√¢ce au multiplicateur de Lagrange optimal $\\boldsymbol \\alpha^\\star$ obtenu avec `qp` :\n",
    "1. Les vecteurs de support $\\mathbf{x}_s$ (tels que $ \\alpha_s^\\star > 0 $)\n",
    "2. Le vecteur normal $\\mathbf{w}^\\star$ de l'hyperplan optimal : $\\displaystyle \\mathbf{w}^\\star = \\sum_{i=1}^{n} \\alpha_i^\\star y_i \\mathbf{x}_i$\n",
    "3. Le biais $ b^\\star$ de l'hyperplan optimal $y_i \\big( (\\mathbf{w}^\\star)^T \\mathbf{x}_i + b^\\star)\\big) = 1$ calcul√© √† partir d'un vecteur de support. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd93154",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.array(result_cvxopt['x']).flatten()\n",
    "\n",
    "# Calcul des vecteurs de support\n",
    "Xs_indices = ??? # FIXME ‚ö†Ô∏è indices (bool√©en) des vecteurs de support\n",
    "ys = ??? # FIXME ‚ö†Ô∏è labels des vecteurs de support\n",
    "Xs = ??? # FIXME ‚ö†Ô∏è coordonn√©es de vecteurs de support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc4432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des param√®tres de l'hyperplan optimal\n",
    "w_cvxopt = ??? # FIXME ‚ö†Ô∏è vecteur normal √† l'hyperplan\n",
    "b_cvxopt = ??? # FIXME ‚ö†Ô∏è biais de l'hyperplan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea2a89",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è üöß üë∑  √Ä vous de jouer !\n",
    "\n",
    "Affichez la solution que vous avez trouv√© pour le SVM avec `qp`. Si celle-ci est correcte (et sous r√©serve que celle que vous avez obtenu avec `minimize` l'√©tait aussi), vous devriez donc obtenir la m√™me chose..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1bab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svm(X,y,Xs,ys,w_cvxopt,b_cvxopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7fd8e0",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è üöß üë∑  √Ä vous de jouer !\n",
    "\n",
    "Sous r√©serve que vous avez trouv√© les bonnes solutions avec `minimize` et `qp`, l'affichage du scatterplot avec les vecteurs de support et hyperplans optimaux doit normalement √™tre le m√™me dans les deux cas de figure (ouf).\n",
    "\n",
    "V√©rifiez donc que les param√®tres $\\mathbf{w}^\\star$ et $b^\\star$ concordent bien num√©riquement pour les deux m√©thodes : \n",
    "- est-ce vraiment le cas ?\n",
    "- que pouvez vous en conclure ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca4b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_sp ??? w_cvxopt # FIXME ‚ö†Ô∏è est-ce que les vecteurs normaux aux hyperplans concordent ?\n",
    "b_sp ??? b_cvxopt # FIXME ‚ö†Ô∏è est-ce que les biais des hyperplans concodent ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a70ed",
   "metadata": {},
   "source": [
    "On peut donc en conclude que ‚ö†Ô∏è FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b757c1",
   "metadata": {},
   "source": [
    "## Bravo ! üëèüçª\n",
    "\n",
    "Vous en avez termin√© avec la r√©solution du SVM √† marge dure. En route maintenant vers les [SVMs √† marge douce](TP_SVM_exo3.ipynb) !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
