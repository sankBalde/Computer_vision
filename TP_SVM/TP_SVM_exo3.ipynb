{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c408f17d",
   "metadata": {},
   "source": [
    "# Les _support vector machines_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da84c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Les imports de base\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508b2cca",
   "metadata": {},
   "source": [
    "## SVM à marge souple\n",
    "\n",
    "Le **SVM à marge souple** (Soft margin SVM) est une extension du SVM à marge dure qui permet de gérer les jeux de données où les classes ne sont pas parfaitement linéairement séparables. En effet, dans de nombreux cas réels, les données sont bruitées ou contiennent des erreurs de classification, rendant une séparation linéaire parfaite impossible, et l'échec par la même occasion d'une tentative de résolution par un SVM à marge dure.\n",
    "\n",
    "Dans le cas du SVM à marge souple, la formulation du problème permet à certains échantillons de se retrouver \"dans la marge\" (entre l'hyperplan optimal $\\mathbf{w}^T \\mathbf{x} + b = 0$ et l'hyperplan $\\mathbf{w}^T \\mathbf{x} + b = \\pm 1$ selon leur classe), voir du mauvais côté de la marge, comme l'illustre la figure ci-dessous $\\downarrow$\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:552/1*CD08yESKvYgyM7pJhCnQeQ.png\" width=\"600\"/>\n",
    "\n",
    "L'objectif (géométrique) reste toujours de maximiser la marge, mais en intégrant ces erreurs potentielles. Pour cela, des **variables de relâchement** $\\xi_i$ (associées à chaque échantillon $\\mathbf{x}_i$) sont introduites dans la formulation du problème pour relacher les contraintes de séparabilité linéaire.\n",
    "\n",
    "Ainsi, les $n$ contraintes $ y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 \\ \\ \\forall i = 1,\\dots,n $ se transforment dans le cas du SVM à marge souple en :\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 - \\xi_i, \\quad \\forall i = 1, \\dots, n \\\\\n",
    "\\xi_i \\geq 0, \\quad \\forall i = 1, \\dots, n\n",
    "\\end{aligned} $$\n",
    "\n",
    "où les variables de relâchement $\\xi_i$ s'interprètent de la manière suivante :\n",
    "\n",
    "- $\\xi_i = 0$ : L'échantillon est correctement classé et se situe à l'extérieur ou sur la frontière de marge.\n",
    "- $0 < \\xi_i < 1$ : L'échantillon est correctement classé, mais il est à l'intérieur de la marge. Il est donc plus proche de l'hyperplan de séparation que les vecteurs de support.\n",
    "- $\\xi_i \\geq 1$ : L'échantillon est mal classé, car il se trouve du mauvais côté de l'hyperplan de séparation. Plus $\\xi_i$ est grand, plus le point est loin de l'hyperplan de son côté incorrect.\n",
    "\n",
    "<img src=\"https://machinelearningcoban.com/assets/20_softmarginsvm/ssvm3.png\" width=\"600\"/>\n",
    "\n",
    "Dans l'illustration ci-dessus $\\uparrow$, $\\xi_1 > 1$ et $\\xi_3 > 1$ puisque les échantillons $\\mathbf{x}_1$ et $\\mathbf{x}_3$ sont du mauvais côté de la marge. En revanche, $0 < \\xi_2 <1$ puisque $\\mathbf{x}_2$ est dans la marge, mais du bon côté de celle ci."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc1a9e",
   "metadata": {},
   "source": [
    "## Formulation du problème primal avec marge souple\n",
    "\n",
    "Les variables de relâchement $\\xi_i$ étant toutes positives, elles jouent le rôle de coûts additionnels qui doivent être pris en compte dans la définition de la fonction objective à minimiser. Ainsi, la formulation du problème primal $(P)$ du SVM à marge souple s'écrit :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{\\mathbf{w} \\in \\mathbb{R}^p, \\ b \\in \\mathbb{R}, \\ \\boldsymbol \\xi \\in \\mathbb{R}^n} \\quad & \\frac{1}{2} \\| \\mathbf{w} \\|^2 + C \\sum_{i=1}^{n} \\xi_i & (P)\\\\\n",
    "\\text{tel que} \\quad & y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 - \\xi_i, & \\forall i = 1, \\dots, n \\\\\n",
    "& \\xi_i \\geq 0, & \\forall i = 1, \\dots, n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "L'hyperparamètre $C$ contrôle la pénalisation des points mal classés. Un $C$ élevé donne plus de poids à la minimisation des erreurs de classification, tandis qu'un $C$ faible favorise une marge plus large, mais permet potentiellement plus d'erreurs de classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d0705",
   "metadata": {},
   "source": [
    "## Formulation du problème dual\n",
    "\n",
    "Tout comme pour le SVM à marge dure, la formulation du problème dual passe par l'écriture des conditions KKT. Le problème primal $(P)$ diffère un peu de celui du SVM à marge dure puisque de nouvelles variables $\\xi_i$ et de nouvelles contraintes $\\xi_i \\geq 0, \\ \\ \\forall i =1, \\dots, n$ ont été introduites. Le Lagrangien va donc les incorporer dans sa formulation, associées à de nouveaux multiplicateurs de Lagrange : \n",
    "\n",
    "$$ \\mathcal{L}(\\mathbf{w},b, \\boldsymbol \\xi, \\boldsymbol \\alpha, \\boldsymbol \\beta) \\mapsto \\frac{1}{2} \\| \\mathbf{w} \\|^2 + C \\sum_{i=1}^{n} \\xi_i  - \\sum_{i=1}^n \\alpha_i \\big(y_i (w^T \\mathbf{x}_i + b) - 1 + \\xi_i \\big) - \\sum_{i=1}^n \\beta_i \\xi_i$$\n",
    "\n",
    "ou\n",
    "- $(\\mathbf{w}$, b, $\\boldsymbol \\xi) \\in \\mathbb{R}^p \\times \\mathbb{R} \\times \\mathbb{R}^n$ sont les variables primales.\n",
    "- $\\boldsymbol \\alpha \\in \\mathbb{R}^n$ est le vecteur de multiplicateurs de Lagrange associé aux $n$ contraintes $y_i (w^T \\mathbf{x}_i + b) \\geq 1 - \\xi_i$\n",
    "- $\\boldsymbol \\beta \\in \\mathbb{R}^n$ est le vecteur de multiplicateurs de Lagrange associé aux $n$ contraintes $\\xi_i \\geq 0$.\n",
    "\n",
    "La stationnarité du Lagrangien donne :\n",
    "- $\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w},b, \\boldsymbol \\xi, \\boldsymbol \\alpha, \\boldsymbol \\beta) = 0 \\Rightarrow$ $\\mathbf{w}^\\star = \\displaystyle \\sum_{i=1}^{n} \\alpha_i^\\star y_i \\mathbf{x}_i$ : le vecteur normal à l'hyperplan optimal est défini comme pour le SVM à marge dure.\n",
    "- $\\displaystyle \\frac{\\partial \\mathcal{L}}{\\partial b}(\\mathbf{w},b, \\boldsymbol \\xi, \\boldsymbol \\alpha, \\boldsymbol \\beta) = 0 \\Rightarrow$ $\\displaystyle \\sum_{i=1}^n \\alpha_i^\\star y_i = 0$ : cette contrainte reste également valable pour la formulation du problème dual.\n",
    "- $\\nabla_{\\boldsymbol \\xi} \\mathcal{L}(\\mathbf{w},b, \\boldsymbol \\xi, \\boldsymbol \\alpha, \\boldsymbol \\beta) = 0 \\Rightarrow$ $\\forall i = 1,\\dots,n \\ \\ \\beta_i^\\star = C - \\alpha_i^\\star \\Leftrightarrow \\alpha_i^\\star \\leq C$ puisque $\\beta_i^\\star \\geq 0$ (admissibilité duale).\n",
    "\n",
    "Cette dernière relation permet d'exprimer $\\boldsymbol \\beta = C - \\boldsymbol \\alpha$ et donc de tout reformuler en fonction de $\\boldsymbol \\alpha$ seulement pour obtenir le problème dual $(D)$ du SVM à marge souple :\n",
    "\n",
    "$$\\begin{align}\n",
    "\\max_{\\boldsymbol \\alpha \\in \\mathbb{R}^{n}} \\quad & \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j (\\mathbf{x}_i^T \\mathbf{x}_j) \\qquad (D) \\\\\n",
    "\\text{tel que} \\quad & 0 \\leq \\alpha_i \\leq C \\ \\ \\forall i = 1, \\dots, n \\\\\n",
    "& \\sum_{i=1}^n \\alpha_i y_i = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "L'expression du dual du SVM à marge souple est ainsi très similaire à celle du SVM à marge dure, la seule différence étant que la contrainte de positivité des multiplicateurs de Lagrange $\\alpha_i \\geq 0 \\ \\ \\forall i = 1, \\dots, n$ se transforme en $0 \\leq \\alpha_i \\leq C \\ \\  \\forall i = 1, \\dots, n$ :\n",
    "- Les multiplicateurs de Lagrange $\\alpha_i$ se retrouvent bornés supérieurement par la valeur de l'hyperparamètre $C$, ce qui veut dire que les vecteurs de support ne peuvent pas \"repousser\" l'hyperplan optimal avec une force supérieure à $C$.\n",
    "- Si $C = +\\infty$, on retrouve le SVM à marge dure, pour lequel il ne peut y avoir de points mal classés !\n",
    "\n",
    "\n",
    "L'écriture sous forme vectorielle du problème dual est donc similaire à celle du SVM à marge dure : \n",
    "\n",
    "$$\\begin{align}\n",
    "\\max_{\\boldsymbol \\alpha \\in \\mathbb{R}^{n}} \\quad & \\mathbf{1}^T \\boldsymbol \\alpha - \\frac{1}{2} \\boldsymbol \\alpha^T \\mathbf{Q} \\boldsymbol \\alpha \\qquad (D)\\\\\n",
    "\\text{tel que} \\quad & 0 \\leq \\alpha_i \\leq C \\ \\ \\forall i = 1, \\dots, n \\\\\n",
    "& \\mathbf{y}^T \\boldsymbol \\alpha = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "avec les mêmes notations que celles de l'[exercice 2](TP_SVM_exo2.ipynb) :\n",
    "- $ \\mathbf{1} \\in \\mathbb{R}^n $ est un vecteur de 1.\n",
    "- $ \\mathbf{Q} \\in \\mathbb{R}^{n \\times n} $ est la matrice des produits scalaires des vecteurs d'entrée multipliés par le produit de leur classe $ Q_{ij} = y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j $.\n",
    "- $ \\mathbf{y} \\in \\mathbb{R}^n $ est le vecteur des labels de classe, avec $ y_i \\in \\{-1, 1\\} $.\n",
    "- $C \\geq 0$ est l'hyperparamètre de régularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a93b6cb",
   "metadata": {},
   "source": [
    "## Solution du problème primal\n",
    "\n",
    "Une fois la solution optimale $ \\boldsymbol \\alpha^\\star $ du problème dual trouvée, on peut en déduire la solution optimale du problème primal de la manière suivante :\n",
    "\n",
    "- **Vecteur normal à l'hyperplan optimal $ \\mathbf{w}^\\star $** (idem que pour le SVM à marge dure) : $$ \\mathbf{w}^\\star = \\sum_{i=1}^{n} \\alpha_i^\\star y_i \\mathbf{x}_i $$\n",
    "- **Vecteurs de support et variables de relâchement $\\xi_i$** : contrairement au SVM à marge dure où il suffisait de seuiller $\\boldsymbol \\alpha^\\star$ pour récupérer les vecteurs de support ($\\alpha_i^\\star > 0 \\Leftrightarrow \\mathbf{x}_i$ est vecteur de support), il est nécessaire de prendre une précaution supplémentaire dans le cas du SVM à marge souple :\n",
    "   - $\\alpha_i^\\star = 0 \\Leftrightarrow$ $\\mathbf{x}_i$ n'est pas vecteur de support $\\Leftrightarrow$ $y_i \\big((\\mathbf{w}^{\\star})^T \\mathbf{x}_i + b^\\star \\big) > 1$ $\\Leftrightarrow$ $\\xi_i^\\star = 0$\n",
    "   - $0 < \\alpha_i^\\star < C \\Leftrightarrow$ $\\mathbf{x}_i$ est vecteur de support $\\Leftrightarrow$ $y_i \\big((\\mathbf{w}^{\\star})^T \\mathbf{x}_i + b^\\star \\big) = 1$ $\\Leftrightarrow$ $\\xi_i^\\star = 0$\n",
    "   - $\\alpha_i^\\star = C \\Leftrightarrow$ $\\mathbf{x}_i$ n'est pas vecteur de support $\\Leftrightarrow$ $y_i \\big((\\mathbf{w}^{\\star})^T \\mathbf{x}_i + b^\\star \\big) < 1$ $\\Leftrightarrow$ $\\xi_i^\\star = 1 - y_i\\big((\\mathbf{w}^{\\star})^T \\mathbf{x}_i + b^\\star \\big)$\n",
    "       - Si $0 < \\xi_i < 1$ : $\\mathbf{x}_i$ est dans la marge, mais du bon côté de l'hyperplan séparateur (donc bien classé).\n",
    "       - Si $\\xi_i = 1$ : $\\mathbf{x}_i$ est sur l'hyperplan séparateur.\n",
    "       - Si $\\xi_i > 1$ : $\\mathbf{x}_i$ est du mauvais côté de l'hyperplan séparateur (donc mal classé).\n",
    "- **Biais de l'hyperplan optimal $ b^\\star $** : Le biais optimal peut être calculé à partir des vecteurs de support (⚠️ à partir de n'importe quel $ \\mathbf{x}_i $ tel que $0 < \\alpha_i^\\star < C $) en utilisant l'équation :\n",
    "  $$\n",
    "  y_i \\big( (\\mathbf{w}^\\star)^T \\mathbf{x}_i + b^\\star)\\big) = 1\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe40424",
   "metadata": {},
   "source": [
    "## Génération d'un jeu de données presque linéairement séparable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270e6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea3c99b",
   "metadata": {},
   "source": [
    "On reprend la génération d'un jeu de données en dimension $2$ de l'exercice précédent, en le modifiant pour qu'il soit cette fois-ci **presque linéairement séparable**.\n",
    "\n",
    "Pour le moment, vous pouvez laisser les deux paramètres `class_sep` (séparation entre les deux classes) et `scale` (écart-type du bruit gaussien rajouté) à leur valeur par défaut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(class_sep=2, scale=1, display=True):\n",
    "    # Génération d'un jeu de données linéairement séparable\n",
    "    X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0,\n",
    "                               n_clusters_per_class=1, flip_y=0, class_sep=class_sep, random_state=42)\n",
    "    # Ajout d'un bruit aux données pour qu'elles ne soient plus séparables\n",
    "    np.random.seed(seed=42)\n",
    "    noise = np.random.normal(scale=scale, size=X.shape)\n",
    "    X += noise\n",
    "    # Relabellisation de la classe 0 → -1\n",
    "    y[y==0] = -1\n",
    "    \n",
    "    # Affichage des données\n",
    "    if display:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(X[y==-1,0], X[y==-1,1], color='red', label='Classe -1', edgecolor='k')\n",
    "        plt.scatter(X[y==1,0], X[y==1,1], color='blue', label='Classe +1', edgecolor='k')\n",
    "        plt.xlabel('$x_1$')\n",
    "        plt.ylabel('$x_2$')\n",
    "        plt.title('Jeu de données presque linéairement séparable')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13849f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = generate_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30df01a",
   "metadata": {},
   "source": [
    "## Résolution du SVM avec marge souple en utilisant `scipy.optimize.minimize`\n",
    "\n",
    "Et c'est parti pour la résolution du problème dual du SVM à marge souple avec la fonction `minimize` de `scipy.optimize` !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ecb278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a02033b",
   "metadata": {},
   "source": [
    "### 🛠️ 🚧 👷  À vous de jouer !\n",
    "\n",
    "Résolvez le problème dual du SVM à marge souple grâce à `minimize`, en utilisant la même démarche que le SVM à marge dure (rappelée ici) :\n",
    "1. Le problème dual $(D)$ étant un problème de **maximisation**, et la fonction `minimize` étant (comme son nom l'indique), une routine permettant de **minimiser** une fonction objective, reformulez tout d'abord le problème dual pour l'écrire comme un problème de minimisation.\n",
    "2. Définissez explicitement la matrice $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$ de terme général $ Q_{ij} = y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j $.\n",
    "3. Définissez la fonction objective à minimiser selon le format attendu par `minimize`.\n",
    "4. Définissez les fonctions de contraintes du problème dual selon le format attendu par `minimize`.<br>\n",
    "<u>Note</u> : la contrainte $0 \\leq \\alpha_i \\leq C$ peut être définie soit via deux contraintes d'inégalité ($0 \\leq \\alpha_i$ et $\\alpha_i \\leq C$), soit grâce à l'argument `bounds`.\n",
    "5. Résolvez numériquement le problème dual grâce à `minimize`.\n",
    "\n",
    "\n",
    "Pour le moment, vous pouvez régler l'hyperparamètre $C = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db61f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition de la matrice Q de l'objective duale\n",
    "Q = ??? # # FIXME ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition de la fonction objective duale\n",
    "def objective_function(x):\n",
    "    ??? # FIXME ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb2673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition des contraintes\n",
    "def eq_constraint:\n",
    "    ??? # FIXME ⚠️\n",
    "\n",
    "def ineq_constraint_0:\n",
    "    ??? # FIXME ⚠️\n",
    "    \n",
    "def ineq_constraint_C:\n",
    "    ??? # FIXME ⚠️\n",
    "    \n",
    "constraints = ??? # FIXME ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc83b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = ??? # FIXME ⚠️ Valeur de l'hyperparamètre C\n",
    "result_sp = minimize(???) # FIXME ⚠️\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Variable duale optimale :\", result_sp.x)\n",
    "print(\"Valeur optimale de la fonction objective :\", result_sp.fun)\n",
    "print(\"Succès de l'optimisation :\", result_sp.success)\n",
    "print(\"Message :\", result_sp.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396d2c1d",
   "metadata": {},
   "source": [
    "### 🛠️ 🚧 👷  À vous de jouer !\n",
    "\n",
    "Une fois le problème dual résolu et le multiplicateur de Lagrange optimal $\\boldsymbol \\alpha^\\star$ obtenu, déterminez :\n",
    "1. Le vecteur normal $\\mathbf{w}^\\star$ à l'hyperplan optimal : $\\displaystyle \\mathbf{w}^\\star = \\sum_{i=1}^{n} \\alpha_i^\\star y_i \\mathbf{x}_i$\n",
    "2. Les vecteurs de support $\\mathbf{x}_s$ (tels que $0 < \\alpha_s^\\star < C$).\n",
    "3. Le biais $ b^\\star$ de l'hyperplan optimal $y_i \\big( (\\mathbf{w}^\\star)^T \\mathbf{x}_i + b^\\star)\\big) = 1$ calculé à partir d'un vecteur de support. \n",
    "4. Les échantillons $\\mathbf{x}_i$ à l'intérieur de la marge ou mal classés (tels que $\\alpha_i^\\star = C $) et leur variable de relâchement associée $\\xi_i^\\star = 1 - y_i\\big((\\mathbf{w}^{\\star})^T \\mathbf{x}_i + b^\\star \\big)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d50aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = result_sp.x\n",
    "\n",
    "# Calcul des vecteurs de support\n",
    "Xs_indices = ??? # FIXME ⚠️ indices (booléen) des vecteurs de support\n",
    "ys = ??? # FIXME ⚠️ labels des vecteurs de support\n",
    "Xs = ??? # FIXME ⚠️ coordonnées de vecteurs de support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f8cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des paramètres de l'hyperplan optimal\n",
    "w_sp = ??? # FIXME ⚠️ vecteur normal à l'hyperplan\n",
    "b_sp = ??? # FIXME ⚠️ biais de l'hyperplan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e0e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des variables de relâchement\n",
    "Xe_indices = ??? # FIXME ⚠️ indices (booléen) des échantillons dans la marge ou mal classés\n",
    "ye = ??? # FIXME ⚠️ labels de ces échantillons erronnés\n",
    "Xe = ??? # FIXME ⚠️ coordonnées de ces échantillons erronnés\n",
    "Xi_e = ??? # FIXME ⚠️ variables de relâchement associées à ces échantillons erronnés"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ea5b02",
   "metadata": {},
   "source": [
    "### Affichage de l'hyperplan optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8526216",
   "metadata": {},
   "source": [
    "Pour finir, on reprend et complète la fonction d'affichage de l'exercice précédent, pour afficher cette fois-ci :\n",
    "- les vecteurs de supports `Xs` et leur classe associée `ys`.\n",
    "- les échantillons dans la marge ou mal classifiés `Xe` et leur classe associée `ye` (la fonction attend également les variables de relâchement `Xi_e` associées).\n",
    "- l'hyperplan optimal du SVM de paramètres $\\mathbf{w}^\\star$ et $b^\\star$.\n",
    "- les hyperplans passant par les vecteurs de support des deux classes (d'équation ${(\\mathbf{w}^\\star)}^T \\mathbf{x} + b^\\star = \\pm 1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d93f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm(X, y, Xs, ys, Xe, ye, Xi_e, w_star, b_star, label=''):\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.grid(True)\n",
    "    # Classe -1\n",
    "    plt.scatter(X[y==-1,0], X[y==-1,1],\n",
    "                color='red',label='Classe -1',marker='o',edgecolor='k')\n",
    "    plt.scatter(Xs[ys==-1,0], Xs[ys==-1,1],\n",
    "                color='red',label='Vecteurs de support -1',marker='o',edgecolors='k',s=150)\n",
    "    # Classe +1\n",
    "    plt.scatter(X[y==1,0], X[y==1,1],color='blue', label='Classe +1',marker='o',edgecolor='k')\n",
    "    plt.scatter(Xs[ys==1,0], Xs[ys==1,1], \n",
    "                color='blue',label='Vecteurs de support +1',marker='o',edgecolors='k',s=150)\n",
    "    # Points dans la marge (0 < xi < 1)\n",
    "    in_margin = (Xi_e > 0) & (Xi_e < 1)\n",
    "    plt.scatter(Xe[in_margin,0], Xe[in_margin,1], facecolors='none', edgecolors='orange', \n",
    "                    s=150, linewidths=2, label='Dans la marge')\n",
    "    # Points mal classés (xi > 1)\n",
    "    misclassified = Xi_e >= 1\n",
    "    plt.scatter(Xe[misclassified,0], Xe[misclassified,1], facecolors='none', edgecolors='purple', \n",
    "                    s=150, linewidths=2, label='Mal classé')   \n",
    "    # Hyperplan optimal\n",
    "    xmin = X[:,0].min()-0.5\n",
    "    xmax = X[:,0].max()+0.5\n",
    "    x_vals = np.linspace(xmin, xmax, 200)\n",
    "    y_vals = -(w_star[0]*x_vals+b_star)/w_star[1]\n",
    "    plt.plot(x_vals, y_vals, 'k-', label='Hyperplan optimal')\n",
    "    # Hyperplans passant par les vecteurs de support (w.x+b=±1)\n",
    "    y_vals_support1 = -(w_star[0]*x_vals+(b_star-1))/w_star[1]\n",
    "    y_vals_support2 = -(w_star[0]*x_vals+(b_star+1))/w_star[1]\n",
    "    plt.plot(x_vals, y_vals_support1, 'k--', label='Hyperplan +1')\n",
    "    plt.plot(x_vals, y_vals_support2, 'k-.', label='Hyperplan -1')\n",
    "    # Titre, labels et légende\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.title('Hyperplan optimal et vecteurs de support ' + label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef3f2b8",
   "metadata": {},
   "source": [
    "### 🛠️ 🚧 👷  À vous de jouer !\n",
    "\n",
    "Affichez la solution que vous avez trouvé pour le SVM. Si celle-ci est correcte, la figure devrait faire sens...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45a0fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svm(X,y,Xs,ys,Xe,ye,Xi_e,w_sp,b_sp,label='(scipy)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42179f8d",
   "metadata": {},
   "source": [
    "## Résolution du SVM avec marge souple en utilisant `cvxopt`\n",
    "\n",
    "Il est maintenant temps de passer à la résolution du SVM à marge souple grâce à la fonction `qp` de `cvxopt`. Le problème dual étant toujours un problème QP, vous allez pouvoir appliquer la même démarche que pour le SVM à marge dure.\n",
    "\n",
    "Pour rappel, le format du programme quadratique résolu par la fonction `qp` est \n",
    "$$\\begin{aligned}\n",
    "& \\text{minimiser} & \\frac{1}{2} \\mathbf{x}^T \\mathbf{P} \\mathbf{x} + \\mathbf{q}^T \\mathbf{x} \\\\\n",
    "& \\text{sous les contraintes} & \\mathbf{G}\\mathbf{x} \\leq \\mathbf{h} \\\\\n",
    "& & \\mathbf{A}\\mathbf{x} = \\mathbf{b}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Il faut donc spécifier au solveur `qp` les différentes matrices $\\mathbf{P}, \\mathbf{G}, \\mathbf{A}$ et vecteurs $\\mathbf{q}, \\mathbf{h}, \\mathbf{b}$ au format `matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19da86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxopt\n",
    "from cvxopt import printing\n",
    "cvxopt.matrix_repr = printing.matrix_str_default\n",
    "from cvxopt import matrix\n",
    "from cvxopt.solvers import qp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cee174",
   "metadata": {},
   "source": [
    "La formulation du problème dual du SVM à marge souple diffère de celle du SVM à marge dure uniquement au niveau des contraintes d'inégalités $0 \\leq \\alpha_i$, transformées en $0 \\leq \\alpha_i \\leq C \\ \\ \\forall i = 1,\\dots,n$. La définition des matrices $\\mathbf{P}$ et $\\mathbf{A}$ et des vecteurs $\\mathbf{q}$ et $\\mathbf{b}$ est donc en tout point similaire à ce que vous avez fait dans l'[exercice précédent](TP_SVM_exo2.ipynb).\n",
    "\n",
    "En ce qui concerne les contraintes d'inégalité $\\mathbf{G}\\mathbf{x} \\leq \\mathbf{h}$, c'est en revanche un peu plus subtil ici...\n",
    "\n",
    "Pour le SMV à marge dure, vous avez (normalement) mis les contraintes $0 \\leq \\alpha_i \\ \\ \\forall i = 1,\\dots,n$ sous la forme $\\, -\\alpha_i \\leq 0 \\ \\ \\forall i = 1, \\dots, n$ écrit sous forme matricielle comme $-\\mathbf{I}_n \\boldsymbol \\alpha \\leq \\mathbf{0}$ avec $\\mathbf{I}_n \\in \\mathbb{R}^{n \\times n}$ la matrice identité de taille $n\\times n$ et $\\mathbf{0}$ un vecteur de $\\mathbb{R}^n$ rempli de 0.\n",
    "\n",
    "Pour le SVM à marge souple en revanche, écrire les contraintes d'inégalités $0 \\leq \\alpha_i \\leq C \\ \\ \\forall i = 1,\\dots,n$ sous la forme standard $\\mathbf{G} \\mathbf{x} \\leq \\mathbf{h}$ attendue par `qp` requiert une petite manipulation :\n",
    "\n",
    "* <u>Cas d'une seule contrainte</u> : $0 \\leq \\alpha \\leq C$ <br>\n",
    "Cette contrainte peut se réécrire $ - \\alpha \\leq 0$ et $\\alpha \\leq C$ $\\Leftrightarrow$ $\\left\\{\\begin{align} - \\alpha & \\leq 0 \\\\ \\alpha &\\leq C\\end{align}\\right.$ $\\Leftrightarrow$ $\\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} \\alpha \\leq \\begin{pmatrix} 0 \\\\ C \\end{pmatrix}$ $\\rightarrow$. Dans ce cas, on identifie $\\mathbf{G} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$ et $\\mathbf{h} = \\begin{pmatrix} 0 \\\\ C \\end{pmatrix}$.<br>\n",
    "<br>\n",
    "* <u>Cas de deux contraintes</u> : $0 \\leq \\alpha_1 \\leq C$ et $0 \\leq \\alpha_2 \\leq C$<br>\n",
    "En appliquant la même idée, on a $\\left\\{\\begin{align} - \\alpha_1 & \\leq 0 \\\\ - \\alpha_2 & \\leq 0 \\\\ \\alpha_1 &\\leq C \\\\ \\alpha_2 & \\leq C\\end{align}\\right.$ $\\ \\Leftrightarrow \\ $ $\\begin{pmatrix} -1  & 0 \\\\ 0 & -1 \\\\ 1 & 0 \\\\ 0 & 1\\end{pmatrix} \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\end{pmatrix} \\leq \\begin{pmatrix} 0 \\\\ 0 \\\\ C \\\\ C\\end{pmatrix}$, ce qui permet là encore d'identifier la matrice $\\mathbf{G}$ et le vecteur $\\mathbf{h}$ attendus par `qp`.<br><br>\n",
    "* <u> Cas de $n$ contraintes</u> : $0 \\leq \\alpha_i \\leq C \\ \\ \\forall i=1,\\dots,n$<br>\n",
    "À vous de généraliser la relation précédente au cas de $n$ contraintes..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e3e533",
   "metadata": {},
   "source": [
    "### 🛠️ 🚧 👷  À vous de jouer !\n",
    "\n",
    "Résolvez le problème dual du SVM à marge souple grâce au solveur `qp` de `cvxopt` en utilisant la même démarche que le SVM à marge dure (rappelée ici) :\n",
    "1. La reformulation du problème dual $(D)$ comme un problème de **minimisation** pour pouvoir appliquer `minimize` reste valable dans le cas de `qp`, qui attend également de **minimiser** une fonction quadratique donnée.\n",
    "2. Identifiez les matrices $\\mathbf{P}$, $\\mathbf{G}$ et $\\mathbf{A}$ et les vecteurs $\\mathbf{q}$, $\\mathbf{h}$, $\\mathbf{b}$ de la forme standard attendue par `qp` à partir de la formulation du problème dual, et en utilisant la manipulation présentée ci-dessus pour $\\mathbf{G}$ et $\\mathbf{h}$) et définissez les au format `matrix` de `cvxopt`.<br>\n",
    "3. Résolvez numériquement le problème dual grâce à `qp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1653114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition des matrices du problème QP\n",
    "C = ??? # FIXME ⚠️ Hyperparamètre C\n",
    "P = ??? # FIXME ⚠️\n",
    "q = ??? # FIXME ⚠️\n",
    "G = ??? # FIXME ⚠️\n",
    "h = ??? # FIXME ⚠️\n",
    "A = ??? # FIXME ⚠️\n",
    "b = ??? # FIXME ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744bc7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Résolution du problème dual\n",
    "result_cvxopt = qp(???) # FIXME ⚠️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5c14d6",
   "metadata": {},
   "source": [
    "### 🛠️ 🚧 👷  À vous de jouer !\n",
    "\n",
    "Une fois le problème dual résolu par `qp` et le multiplicateur de Lagrange optimal $\\boldsymbol \\alpha^\\star$ obtenu, déterminez :\n",
    "1. Le vecteur normal $\\mathbf{w}^\\star$ à l'hyperplan optimal : $\\displaystyle \\mathbf{w}^\\star = \\sum_{i=1}^{n} \\alpha_i^\\star y_i \\mathbf{x}_i$\n",
    "2. Les vecteurs de support $\\mathbf{x}_s$ (tels que $0 < \\alpha_s^\\star < C$).\n",
    "3. Le biais $ b^\\star$ de l'hyperplan optimal $y_i \\big( (\\mathbf{w}^\\star)^T \\mathbf{x}_i + b^\\star)\\big) = 1$ calculé à partir d'un vecteur de support. \n",
    "4. Les échantillons $\\mathbf{x}_i$ à l'intérieur de la marge ou mal classés (tels que $\\alpha_i^\\star = C $) et leur variable de relâchement associée $\\xi_i^\\star = 1 - y_i\\big((\\mathbf{w}^{\\star})^T \\mathbf{x}_i + b^\\star \\big)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d2da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.array(result_cvxopt['x']).flatten()\n",
    "\n",
    "# Calcul des vecteurs de support\n",
    "Xs_indices = ??? # FIXME ⚠️ indices (booléen) des vecteurs de support\n",
    "ys = ??? # FIXME ⚠️ labels des vecteurs de support\n",
    "Xs = ??? # FIXME ⚠️ coordonnées de vecteurs de support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7783d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des paramètres de l'hyperplan optimal\n",
    "w_cvxopt = ??? # FIXME ⚠️ vecteur normal à l'hyperplan\n",
    "b_cvxopt = ??? # FIXME ⚠️ biais de l'hyperplan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223c80e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des variables de relâchement\n",
    "Xe_indices = ??? # FIXME ⚠️ indices (booléen) des échantillons dans la marge ou mal classés\n",
    "ye = ??? # FIXME ⚠️ labels de ces échantillons erronnés\n",
    "Xe = ??? # FIXME ⚠️ coordonnées de ces échantillons erronnés\n",
    "Xi_e = ??? # FIXME ⚠️ variables de relâchement associées à ces échantillons erronnés"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7a71f5",
   "metadata": {},
   "source": [
    "### 🛠️ 🚧 👷  À vous de jouer !\n",
    "\n",
    "Pour finir, affichez la solution que vous avez trouvé pour le SVM avec le dual résolu par `qp`. Si celle-ci est correcte, la figure devrait donc être identique à celle obtenue pour `minimize` (sous réserve que celle-ci aussi soit correcte bien sur...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1bab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svm(X,y,Xs,ys,Xe,ye,Xi_e,w_cvxopt,b_cvxopt,label='(cvxopt)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b424aa1",
   "metadata": {},
   "source": [
    "## Un peu de lecture 📖 : résolution du SVM à marge souple en utilisant `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b546429",
   "metadata": {},
   "source": [
    "Quand on parle de SVM (donc d'un problème de classification), on pense en général à `scikit-learn`. Votre bibliothèque favorite de _machine learning_ implémente évidemment le SVM. Il serait logique que le résultat de `scikit-learn` soit identique à ceux de `minimize` et `qp`.\n",
    "\n",
    "L'API de `scikit-learn` est bien différente de celles de `minimize` et `qp` puisqu'elle se focalise sur les aspects de _machine learning_ (en même temps, c'est ce qu'on attend d'elle) plutôt que les aspects d'optimisation. On peut malgré tout s'en sortir en fouillant un peu les différents attributs du modèle de SVM implémenté dans `scikit-learn` : c'est ce qu'il vous est proposé de lire dans cette partie (votre prof a fait le boulot à votre place 😉)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2edd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e7e03",
   "metadata": {},
   "source": [
    "Définition du modèle de SVM linéaire (_aka_ pas la version noyau ici) avec l'hyperparamètre $C$ défini à la même valeur que pour `minimize` et `qp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10dde11",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = SVC(kernel='linear',C=C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b201b2b",
   "metadata": {},
   "source": [
    "Entraînement du SVM sur l'intégralité du jeu de données (pas besoin de train/test split ici puisqu'on ne va pas tester les performances en prédiction du modèle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d3fb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c806ceb6",
   "metadata": {},
   "source": [
    "Une fois l'entraînement réalisé, on peut récupérer le vecteur normal à l'hyperplan optimal grâce à l'attribut `coef_` et le biais de l'hyperplan grâce à l'attribut `intercept_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed61f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vecteur normal à l'hyperplan optimal :\",SVM.coef_.flatten()) # flatten() pour avoir une shape (2,)\n",
    "print(\"Biais de l'hyperplan optimal :\",SVM.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e4a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_sk = SVM.coef_.flatten()\n",
    "b_sk = SVM.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75a15af",
   "metadata": {},
   "source": [
    "L'API de `scikit-learn` ne permet pas d'accéder aux multiplicateurs de Lagrange optimaux $\\alpha_i^\\star$ (puisqu'encore une fois, ils n'ont pas d'intérêt en soit pour des opérations pratiques de _machine learning_). Elle permet en revanche d'accéder aux indices des vecteurs de support par l'attribut `support_` et à leurs coordonnées `support_vectors_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfbdca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Indice des vecteurs de support :\",SVM.support_)\n",
    "print(\"Coordonnées des vecteurs de support :\\n\",SVM.support_vectors_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883b897f",
   "metadata": {},
   "source": [
    "⚠️ Pour `scikit-learn`, les vecteurs de support ne sont pas uniquement les échantillons qui vérifient $y_i(\\mathbf{w}^T \\mathbf{x}_i + b) = 1$ (donc sur les hyperplans $\\pm 1$), mais également ceux qui tombent dans la marge ou du mauvais côté de l'hyperplan optimal : en bref, pour `scikit-learn`, les vecteurs de support sont ceux dont le multiplicateur de Lagrange associé est strictement positif $\\alpha_i^\\star > 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775ba6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_indices_all = SVM.support_ # indice de tous les vecteurs de support pour scikit-learn\n",
    "ys_all = y[Xs_indices_all] # labels de ces vecteurs de support\n",
    "Xs_all = X[Xs_indices_all,:] # coordonnées de ces vecteurs de support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b277e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.grid(True)\n",
    "# Classe -1\n",
    "plt.scatter(Xs_all[ys_all==-1,0], Xs_all[ys_all==-1,1],\n",
    "            color='red',label='Classe -1',marker='o',edgecolor='k')\n",
    "# Classe +1\n",
    "plt.scatter(Xs_all[ys_all==1,0], Xs_all[ys_all==1,1],\n",
    "            color='blue', label='Classe +1',marker='o',edgecolor='k')\n",
    "# Hyperplan optimal\n",
    "xmin = Xs_all[:,0].min()-0.25\n",
    "xmax = Xs_all[:,0].max()+0.25\n",
    "x_vals = np.linspace(xmin, xmax, 200)\n",
    "y_vals = -(w_sk[0]*x_vals+b_sk)/w_sk[1]\n",
    "plt.plot(x_vals, y_vals, 'k-', label='Hyperplan optimal')\n",
    "# Hyperplans passant par les vecteurs de support (w.x+b=±1)\n",
    "y_vals_support1 = -(w_sk[0]*x_vals+(b_sk-1))/w_sk[1]\n",
    "y_vals_support2 = -(w_sk[0]*x_vals+(b_sk+1))/w_sk[1]\n",
    "plt.plot(x_vals, y_vals_support1, 'k--', label='Hyperplan +1')\n",
    "plt.plot(x_vals, y_vals_support2, 'k-.', label='Hyperplan -1')\n",
    "# Titre, labels et légende\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('Hyperplan optimal et vecteurs de support de scikit-learn')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f4e6cb",
   "metadata": {},
   "source": [
    "Pour identifier les \"vrais\" vecteurs de support au sens où on l'entend depuis le début, à savoir ceux dont le multiplicateur de Lagrange optimal est $0 < \\alpha_i^\\star < C$, il suffit donc de chercher parmis ces points ceux qui vérifient l'équation $y_i(\\mathbf{w}^T \\mathbf{x}_i + b) = 1$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edba553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_all*(np.dot(Xs_all,w_sk) + b_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b101e21c",
   "metadata": {},
   "source": [
    "⚠️ La précision numérique du calcul précédent est telle qu'un seuil dur `== 1` est voué à l'échec. En revanche, une tolérance de $10^{-3}$ fera l'affaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffe852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_indices = np.isclose(ys_all*(np.dot(Xs_all,w_sk) + b_sk),1,atol=1e-3)\n",
    "ys = ys_all[Xs_indices]\n",
    "Xs = Xs_all[Xs_indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9310039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coordonnées des 'vrais' vecteurs de support :\\n\",Xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c769332",
   "metadata": {},
   "source": [
    "Ne reste qu'à récupérer les coordonnées `Xe` des vecteurs dans la marge ou mal classés, leur label `ye` ainsi que la variable de relâchement `Xi_e` qui leur est associée.\n",
    "\n",
    "Cette dernière se calcule via la relation $\\xi_i^\\star = 1 - y_i\\big((\\mathbf{w}^{\\star})^T \\mathbf{x}_i + b^\\star \\big)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9b575",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xe = Xs_all[~Xs_indices,:]\n",
    "ye = ys_all[~Xs_indices]\n",
    "Xi_e = 1 - ye*(np.dot(Xe, w_sk)+b_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4048ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variables de relâchement pour les échantillons dans la marge / mal classés :\\n\",Xi_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b2725f",
   "metadata": {},
   "source": [
    "On peut d'ailleurs vérifier numériquement que ces variables de relâchement $\\xi_i^\\star$ sont toutes strictement positives (encore heureux puisque c'était l'une des contraintes du problème primal...).<br>\n",
    "Les échantillons pour lesquels $0 < \\xi_i^\\star < 1$ sont dans la marge mais du bon côté de l'hyperplan séparateur. Ceux pour lesquels $\\xi_i^\\star > 1$ sont du mauvais côté de l'hyperplan séparateur.\n",
    "\n",
    "\n",
    "Pour finir, on peut afficher le scatterplot des échantillons avec la solution du SVM à marge souple calculée par `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d482cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svm(X,y,Xs,ys,Xe,ye,Xi_e,w_sk,b_sk,label='(scikit-learn)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae1617c",
   "metadata": {},
   "source": [
    "Cette dernière concorde bien visuellement avec les solution trouvées par `minimize` et `qp` (évidemment sous réserve que ce que vous avez fait est correct...)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7fd8e0",
   "metadata": {},
   "source": [
    "### 🛠️ 🚧 👷  À vous de jouer !\n",
    "\n",
    "Pour finir, vérifiez que les paramètres $\\mathbf{w}^\\star$ et $b^\\star$ trouvés par `minimize`, `qp` et `scikit-learn` concordent bien numériquement : \n",
    "- est-ce vraiment le cas ?\n",
    "- que pouvez vous en conclure ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca4b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_sp ??? w_cvxopt # FIXME ⚠️ est-ce que les vecteurs normaux aux hyperplans concordent ?\n",
    "w_sp ??? w_sk # FIXME ⚠️\n",
    "w_sk ??? w_cvxopt # FIXME ⚠️\n",
    "b_sp ??? b_cvxopt # FIXME ⚠️ est-ce que les biais des hyperplans concodent ?\n",
    "b_sp ??? b_sk # FIXME ⚠️\n",
    "b_sk ??? b_cvxopt # FIXME ⚠️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a70ed",
   "metadata": {},
   "source": [
    "On peut donc en conclude que ⚠️ FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157fc8ef",
   "metadata": {},
   "source": [
    "## BENCH BENCH BENCH !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45535b3b",
   "metadata": {},
   "source": [
    "### 🛠️ 🚧 👷  À vous de jouer !\n",
    "\n",
    "Il ne vous aura sans doute pas échappé que dans cet exercice, vous avez résolu le SVM à marge souple pour une seule configuration donnée d'échantillons \"presque linéairement séparables\", et pour une seule valeur de l'hyperparamètre $C=1$.\n",
    "\n",
    "S'il vous reste encore du temps (et de l'énergie), vous pouvez maintenant benchmarker (comme à vos plus belles heures d'OCVX1) le comportement du SVM à marge souple en faisant varier la séparabilité du jeu de données (en modifiant la valeur de `classe_sep` et de `scale`) et/ou la valeur de l'hyperparamètre $C$ pour voir leur influence sur les critères de votre choix, comme par exemple :\n",
    "- la valeur de la marge normalisée $\\frac{2}{\\| \\mathbf{w} \\|}$\n",
    "- la valeur de la pénalisation $\\displaystyle \\sum_{i= 1}^n \\xi_i$ de la fonction objective\n",
    "- la valeur de la fonction objective elle même $\\frac{1}{2} \\| \\mathbf{w} \\|^2 + C \\sum_{i=1}^{n} \\xi_i$\n",
    "- le nombre d'échantillons tombant dans la marge ou mal classifiés\n",
    "- le nombre d'itérations nécessaires au solveur pour converger vers la solution\n",
    "- etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6a03d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME ⚠️ BENCH BENCH BENCH !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1b89b8",
   "metadata": {},
   "source": [
    "# Bravo ! 👏🍾\n",
    "\n",
    "Et bé ! Vous en êtes arrivés au bout, c'était quand même un sacré morceau ce TP ! Bravo à vous 🍻"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
