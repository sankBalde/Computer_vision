{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c408f17d",
   "metadata": {},
   "source": [
    "# Les _support vector machines_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da84c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Les imports de base\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508b2cca",
   "metadata": {},
   "source": [
    "## SVM √† marge souple\n",
    "\n",
    "Le **SVM √† marge souple** (Soft margin SVM) est une extension du SVM √† marge dure qui permet de g√©rer les jeux de donn√©es o√π les classes ne sont pas parfaitement lin√©airement s√©parables. En effet, dans de nombreux cas r√©els, les donn√©es sont bruit√©es ou contiennent des erreurs de classification, rendant une s√©paration lin√©aire parfaite impossible, et l'√©chec par la m√™me occasion d'une tentative de r√©solution par un SVM √† marge dure.\n",
    "\n",
    "Dans le cas du SVM √† marge souple, la formulation du probl√®me permet √† certains √©chantillons de se retrouver \"dans la marge\" (entre l'hyperplan optimal $\\mathbf{w}^T \\mathbf{x} + b = 0$ et l'hyperplan $\\mathbf{w}^T \\mathbf{x} + b = \\pm 1$ selon leur classe), voir du mauvais c√¥t√© de la marge, comme l'illustre la figure ci-dessous $\\downarrow$\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:552/1*CD08yESKvYgyM7pJhCnQeQ.png\" width=\"600\"/>\n",
    "\n",
    "L'objectif (g√©om√©trique) reste toujours de maximiser la marge, mais en int√©grant ces erreurs potentielles. Pour cela, des **variables de rel√¢chement** $\\xi_i$ (associ√©es √† chaque √©chantillon $\\mathbf{x}_i$) sont introduites dans la formulation du probl√®me pour relacher les contraintes de s√©parabilit√© lin√©aire.\n",
    "\n",
    "Ainsi, les $n$ contraintes $ y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 \\ \\ \\forall i = 1,\\dots,n $ se transforment dans le cas du SVM √† marge souple en :\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 - \\xi_i, \\quad \\forall i = 1, \\dots, n \\\\\n",
    "\\xi_i \\geq 0, \\quad \\forall i = 1, \\dots, n\n",
    "\\end{aligned} $$\n",
    "\n",
    "o√π les variables de rel√¢chement $\\xi_i$ s'interpr√®tent de la mani√®re suivante :\n",
    "\n",
    "- $\\xi_i = 0$ : L'√©chantillon est correctement class√© et se situe √† l'ext√©rieur ou sur la fronti√®re de marge.\n",
    "- $0 < \\xi_i < 1$ : L'√©chantillon est correctement class√©, mais il est √† l'int√©rieur de la marge. Il est donc plus proche de l'hyperplan de s√©paration que les vecteurs de support.\n",
    "- $\\xi_i \\geq 1$ : L'√©chantillon est mal class√©, car il se trouve du mauvais c√¥t√© de l'hyperplan de s√©paration. Plus $\\xi_i$ est grand, plus le point est loin de l'hyperplan de son c√¥t√© incorrect.\n",
    "\n",
    "<img src=\"https://machinelearningcoban.com/assets/20_softmarginsvm/ssvm3.png\" width=\"600\"/>\n",
    "\n",
    "Dans l'illustration ci-dessus $\\uparrow$, $\\xi_1 > 1$ et $\\xi_3 > 1$ puisque les √©chantillons $\\mathbf{x}_1$ et $\\mathbf{x}_3$ sont du mauvais c√¥t√© de la marge. En revanche, $0 < \\xi_2 <1$ puisque $\\mathbf{x}_2$ est dans la marge, mais du bon c√¥t√© de celle ci."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc1a9e",
   "metadata": {},
   "source": [
    "## Formulation du probl√®me primal avec marge souple\n",
    "\n",
    "Les variables de rel√¢chement $\\xi_i$ √©tant toutes positives, elles jouent le r√¥le de co√ªts additionnels qui doivent √™tre pris en compte dans la d√©finition de la fonction objective √† minimiser. Ainsi, la formulation du probl√®me primal $(P)$ du SVM √† marge souple s'√©crit :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{\\mathbf{w} \\in \\mathbb{R}^p, \\ b \\in \\mathbb{R}, \\ \\boldsymbol \\xi \\in \\mathbb{R}^n} \\quad & \\frac{1}{2} \\| \\mathbf{w} \\|^2 + C \\sum_{i=1}^{n} \\xi_i & (P)\\\\\n",
    "\\text{tel que} \\quad & y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 - \\xi_i, & \\forall i = 1, \\dots, n \\\\\n",
    "& \\xi_i \\geq 0, & \\forall i = 1, \\dots, n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "L'hyperparam√®tre $C$ contr√¥le la p√©nalisation des points mal class√©s. Un $C$ √©lev√© donne plus de poids √† la minimisation des erreurs de classification, tandis qu'un $C$ faible favorise une marge plus large, mais permet potentiellement plus d'erreurs de classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d0705",
   "metadata": {},
   "source": [
    "## Formulation du probl√®me dual\n",
    "\n",
    "Tout comme pour le SVM √† marge dure, la formulation du probl√®me dual passe par l'√©criture des conditions KKT. Le probl√®me primal $(P)$ diff√®re un peu de celui du SVM √† marge dure puisque de nouvelles variables $\\xi_i$ et de nouvelles contraintes $\\xi_i \\geq 0, \\ \\ \\forall i =1, \\dots, n$ ont √©t√© introduites. Le Lagrangien va donc les incorporer dans sa formulation, associ√©es √† de nouveaux multiplicateurs de Lagrange : \n",
    "\n",
    "$$ \\mathcal{L}(\\mathbf{w},b, \\boldsymbol \\xi, \\boldsymbol \\alpha, \\boldsymbol \\beta) \\mapsto \\frac{1}{2} \\| \\mathbf{w} \\|^2 + C \\sum_{i=1}^{n} \\xi_i  - \\sum_{i=1}^n \\alpha_i \\big(y_i (w^T \\mathbf{x}_i + b) - 1 + \\xi_i \\big) - \\sum_{i=1}^n \\beta_i \\xi_i$$\n",
    "\n",
    "ou\n",
    "- $(\\mathbf{w}$, b, $\\boldsymbol \\xi) \\in \\mathbb{R}^p \\times \\mathbb{R} \\times \\mathbb{R}^n$ sont les variables primales.\n",
    "- $\\boldsymbol \\alpha \\in \\mathbb{R}^n$ est le vecteur de multiplicateurs de Lagrange associ√© aux $n$ contraintes $y_i (w^T \\mathbf{x}_i + b) \\geq 1 - \\xi_i$\n",
    "- $\\boldsymbol \\beta \\in \\mathbb{R}^n$ est le vecteur de multiplicateurs de Lagrange associ√© aux $n$ contraintes $\\xi_i \\geq 0$.\n",
    "\n",
    "La stationnarit√© du Lagrangien donne :\n",
    "- $\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w},b, \\boldsymbol \\xi, \\boldsymbol \\alpha, \\boldsymbol \\beta) = 0 \\Rightarrow$ $\\mathbf{w}^\\star = \\displaystyle \\sum_{i=1}^{n} \\alpha_i^\\star y_i \\mathbf{x}_i$ : le vecteur normal √† l'hyperplan optimal est d√©fini comme pour le SVM √† marge dure.\n",
    "- $\\displaystyle \\frac{\\partial \\mathcal{L}}{\\partial b}(\\mathbf{w},b, \\boldsymbol \\xi, \\boldsymbol \\alpha, \\boldsymbol \\beta) = 0 \\Rightarrow$ $\\displaystyle \\sum_{i=1}^n \\alpha_i^\\star y_i = 0$ : cette contrainte reste √©galement valable pour la formulation du probl√®me dual.\n",
    "- $\\nabla_{\\boldsymbol \\xi} \\mathcal{L}(\\mathbf{w},b, \\boldsymbol \\xi, \\boldsymbol \\alpha, \\boldsymbol \\beta) = 0 \\Rightarrow$ $\\forall i = 1,\\dots,n \\ \\ \\beta_i^\\star = C - \\alpha_i^\\star \\Leftrightarrow \\alpha_i^\\star \\leq C$ puisque $\\beta_i^\\star \\geq 0$ (admissibilit√© duale).\n",
    "\n",
    "Cette derni√®re relation permet d'exprimer $\\boldsymbol \\beta = C - \\boldsymbol \\alpha$ et donc de tout reformuler en fonction de $\\boldsymbol \\alpha$ seulement pour obtenir le probl√®me dual $(D)$ du SVM √† marge souple :\n",
    "\n",
    "$$\\begin{align}\n",
    "\\max_{\\boldsymbol \\alpha \\in \\mathbb{R}^{n}} \\quad & \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j (\\mathbf{x}_i^T \\mathbf{x}_j) \\qquad (D) \\\\\n",
    "\\text{tel que} \\quad & 0 \\leq \\alpha_i \\leq C \\ \\ \\forall i = 1, \\dots, n \\\\\n",
    "& \\sum_{i=1}^n \\alpha_i y_i = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "L'expression du dual du SVM √† marge souple est ainsi tr√®s similaire √† celle du SVM √† marge dure, la seule diff√©rence √©tant que la contrainte de positivit√© des multiplicateurs de Lagrange $\\alpha_i \\geq 0 \\ \\ \\forall i = 1, \\dots, n$ se transforme en $0 \\leq \\alpha_i \\leq C \\ \\  \\forall i = 1, \\dots, n$ :\n",
    "- Les multiplicateurs de Lagrange $\\alpha_i$ se retrouvent born√©s sup√©rieurement par la valeur de l'hyperparam√®tre $C$, ce qui veut dire que les vecteurs de support ne peuvent pas \"repousser\" l'hyperplan optimal avec une force sup√©rieure √† $C$.\n",
    "- Si $C = +\\infty$, on retrouve le SVM √† marge dure, pour lequel il ne peut y avoir de points mal class√©s !\n",
    "\n",
    "\n",
    "L'√©criture sous forme vectorielle du probl√®me dual est donc similaire √† celle du SVM √† marge dure : \n",
    "\n",
    "$$\\begin{align}\n",
    "\\max_{\\boldsymbol \\alpha \\in \\mathbb{R}^{n}} \\quad & \\mathbf{1}^T \\boldsymbol \\alpha - \\frac{1}{2} \\boldsymbol \\alpha^T \\mathbf{Q} \\boldsymbol \\alpha \\qquad (D)\\\\\n",
    "\\text{tel que} \\quad & 0 \\leq \\alpha_i \\leq C \\ \\ \\forall i = 1, \\dots, n \\\\\n",
    "& \\mathbf{y}^T \\boldsymbol \\alpha = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "avec les m√™mes notations que celles de l'[exercice 2](TP_SVM_exo2.ipynb) :\n",
    "- $ \\mathbf{1} \\in \\mathbb{R}^n $ est un vecteur de 1.\n",
    "- $ \\mathbf{Q} \\in \\mathbb{R}^{n \\times n} $ est la matrice des produits scalaires des vecteurs d'entr√©e multipli√©s par le produit de leur classe $ Q_{ij} = y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j $.\n",
    "- $ \\mathbf{y} \\in \\mathbb{R}^n $ est le vecteur des labels de classe, avec $ y_i \\in \\{-1, 1\\} $.\n",
    "- $C \\geq 0$ est l'hyperparam√®tre de r√©gularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a93b6cb",
   "metadata": {},
   "source": [
    "## Solution du probl√®me primal\n",
    "\n",
    "Une fois la solution optimale $ \\boldsymbol \\alpha^\\star $ du probl√®me dual trouv√©e, on peut en d√©duire la solution optimale du probl√®me primal de la mani√®re suivante :\n",
    "\n",
    "- **Vecteur normal √† l'hyperplan optimal $ \\mathbf{w}^\\star $** (idem que pour le SVM √† marge dure) : $$ \\mathbf{w}^\\star = \\sum_{i=1}^{n} \\alpha_i^\\star y_i \\mathbf{x}_i $$\n",
    "- **Vecteurs de support et variables de rel√¢chement $\\xi_i$** : contrairement au SVM √† marge dure o√π il suffisait de seuiller $\\boldsymbol \\alpha^\\star$ pour r√©cup√©rer les vecteurs de support ($\\alpha_i^\\star > 0 \\Leftrightarrow \\mathbf{x}_i$ est vecteur de support), il est n√©cessaire de prendre une pr√©caution suppl√©mentaire dans le cas du SVM √† marge souple :\n",
    "   - $\\alpha_i^\\star = 0 \\Leftrightarrow$ $\\mathbf{x}_i$ n'est pas vecteur de support $\\Leftrightarrow$ $y_i \\big((\\mathbf{w}^{\\star})^T \\mathbf{x}_i + b^\\star \\big) > 1$ $\\Leftrightarrow$ $\\xi_i^\\star = 0$\n",
    "   - $0 < \\alpha_i^\\star < C \\Leftrightarrow$ $\\mathbf{x}_i$ est vecteur de support $\\Leftrightarrow$ $y_i \\big((\\mathbf{w}^{\\star})^T \\mathbf{x}_i + b^\\star \\big) = 1$ $\\Leftrightarrow$ $\\xi_i^\\star = 0$\n",
    "   - $\\alpha_i^\\star = C \\Leftrightarrow$ $\\mathbf{x}_i$ n'est pas vecteur de support $\\Leftrightarrow$ $y_i \\big((\\mathbf{w}^{\\star})^T \\mathbf{x}_i + b^\\star \\big) < 1$ $\\Leftrightarrow$ $\\xi_i^\\star = 1 - y_i\\big((\\mathbf{w}^{\\star})^T \\mathbf{x}_i + b^\\star \\big)$\n",
    "       - Si $0 < \\xi_i < 1$ : $\\mathbf{x}_i$ est dans la marge, mais du bon c√¥t√© de l'hyperplan s√©parateur (donc bien class√©).\n",
    "       - Si $\\xi_i = 1$ : $\\mathbf{x}_i$ est sur l'hyperplan s√©parateur.\n",
    "       - Si $\\xi_i > 1$ : $\\mathbf{x}_i$ est du mauvais c√¥t√© de l'hyperplan s√©parateur (donc mal class√©).\n",
    "- **Biais de l'hyperplan optimal $ b^\\star $** : Le biais optimal peut √™tre calcul√© √† partir des vecteurs de support (‚ö†Ô∏è √† partir de n'importe quel $ \\mathbf{x}_i $ tel que $0 < \\alpha_i^\\star < C $) en utilisant l'√©quation :\n",
    "  $$\n",
    "  y_i \\big( (\\mathbf{w}^\\star)^T \\mathbf{x}_i + b^\\star)\\big) = 1\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe40424",
   "metadata": {},
   "source": [
    "## G√©n√©ration d'un jeu de donn√©es presque lin√©airement s√©parable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270e6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea3c99b",
   "metadata": {},
   "source": [
    "On reprend la g√©n√©ration d'un jeu de donn√©es en dimension $2$ de l'exercice pr√©c√©dent, en le modifiant pour qu'il soit cette fois-ci **presque lin√©airement s√©parable**.\n",
    "\n",
    "Pour le moment, vous pouvez laisser les deux param√®tres `class_sep` (s√©paration entre les deux classes) et `scale` (√©cart-type du bruit gaussien rajout√©) √† leur valeur par d√©faut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(class_sep=2, scale=1, display=True):\n",
    "    # G√©n√©ration d'un jeu de donn√©es lin√©airement s√©parable\n",
    "    X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0,\n",
    "                               n_clusters_per_class=1, flip_y=0, class_sep=class_sep, random_state=42)\n",
    "    # Ajout d'un bruit aux donn√©es pour qu'elles ne soient plus s√©parables\n",
    "    np.random.seed(seed=42)\n",
    "    noise = np.random.normal(scale=scale, size=X.shape)\n",
    "    X += noise\n",
    "    # Relabellisation de la classe 0 ‚Üí -1\n",
    "    y[y==0] = -1\n",
    "    \n",
    "    # Affichage des donn√©es\n",
    "    if display:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(X[y==-1,0], X[y==-1,1], color='red', label='Classe -1', edgecolor='k')\n",
    "        plt.scatter(X[y==1,0], X[y==1,1], color='blue', label='Classe +1', edgecolor='k')\n",
    "        plt.xlabel('$x_1$')\n",
    "        plt.ylabel('$x_2$')\n",
    "        plt.title('Jeu de donn√©es presque lin√©airement s√©parable')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13849f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = generate_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30df01a",
   "metadata": {},
   "source": [
    "## R√©solution du SVM avec marge souple en utilisant `scipy.optimize.minimize`\n",
    "\n",
    "Et c'est parti pour la r√©solution du probl√®me dual du SVM √† marge souple avec la fonction `minimize` de `scipy.optimize` !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ecb278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a02033b",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è üöß üë∑  √Ä vous de jouer !\n",
    "\n",
    "R√©solvez le probl√®me dual du SVM √† marge souple gr√¢ce √† `minimize`, en utilisant la m√™me d√©marche que le SVM √† marge dure (rappel√©e ici) :\n",
    "1. Le probl√®me dual $(D)$ √©tant un probl√®me de **maximisation**, et la fonction `minimize` √©tant (comme son nom l'indique), une routine permettant de **minimiser** une fonction objective, reformulez tout d'abord le probl√®me dual pour l'√©crire comme un probl√®me de minimisation.\n",
    "2. D√©finissez explicitement la matrice $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$ de terme g√©n√©ral $ Q_{ij} = y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j $.\n",
    "3. D√©finissez la fonction objective √† minimiser selon le format attendu par `minimize`.\n",
    "4. D√©finissez les fonctions de contraintes du probl√®me dual selon le format attendu par `minimize`.<br>\n",
    "<u>Note</u> : la contrainte $0 \\leq \\alpha_i \\leq C$ peut √™tre d√©finie soit via deux contraintes d'in√©galit√© ($0 \\leq \\alpha_i$ et $\\alpha_i \\leq C$), soit gr√¢ce √† l'argument `bounds`.\n",
    "5. R√©solvez num√©riquement le probl√®me dual gr√¢ce √† `minimize`.\n",
    "\n",
    "\n",
    "Pour le moment, vous pouvez r√©gler l'hyperparam√®tre $C = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db61f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d√©finition de la matrice Q de l'objective duale\n",
    "Q = ??? # # FIXME ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d√©finition de la fonction objective duale\n",
    "def objective_function(x):\n",
    "    ??? # FIXME ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb2673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition des contraintes\n",
    "def eq_constraint:\n",
    "    ??? # FIXME ‚ö†Ô∏è\n",
    "\n",
    "def ineq_constraint_0:\n",
    "    ??? # FIXME ‚ö†Ô∏è\n",
    "    \n",
    "def ineq_constraint_C:\n",
    "    ??? # FIXME ‚ö†Ô∏è\n",
    "    \n",
    "constraints = ??? # FIXME ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc83b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = ??? # FIXME ‚ö†Ô∏è Valeur de l'hyperparam√®tre C\n",
    "result_sp = minimize(???) # FIXME ‚ö†Ô∏è\n",
    "\n",
    "# Affichage des r√©sultats\n",
    "print(\"Variable duale optimale :\", result_sp.x)\n",
    "print(\"Valeur optimale de la fonction objective :\", result_sp.fun)\n",
    "print(\"Succ√®s de l'optimisation :\", result_sp.success)\n",
    "print(\"Message :\", result_sp.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396d2c1d",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è üöß üë∑  √Ä vous de jouer !\n",
    "\n",
    "Une fois le probl√®me dual r√©solu et le multiplicateur de Lagrange optimal $\\boldsymbol \\alpha^\\star$ obtenu, d√©terminez :\n",
    "1. Le vecteur normal $\\mathbf{w}^\\star$ √† l'hyperplan optimal : $\\displaystyle \\mathbf{w}^\\star = \\sum_{i=1}^{n} \\alpha_i^\\star y_i \\mathbf{x}_i$\n",
    "2. Les vecteurs de support $\\mathbf{x}_s$ (tels que $0 < \\alpha_s^\\star < C$).\n",
    "3. Le biais $ b^\\star$ de l'hyperplan optimal $y_i \\big( (\\mathbf{w}^\\star)^T \\mathbf{x}_i + b^\\star)\\big) = 1$ calcul√© √† partir d'un vecteur de support. \n",
    "4. Les √©chantillons $\\mathbf{x}_i$ √† l'int√©rieur de la marge ou mal class√©s (tels que $\\alpha_i^\\star = C $) et leur variable de rel√¢chement associ√©e $\\xi_i^\\star = 1 - y_i\\big((\\mathbf{w}^{\\star})^T \\mathbf{x}_i + b^\\star \\big)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d50aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = result_sp.x\n",
    "\n",
    "# Calcul des vecteurs de support\n",
    "Xs_indices = ??? # FIXME ‚ö†Ô∏è indices (bool√©en) des vecteurs de support\n",
    "ys = ??? # FIXME ‚ö†Ô∏è labels des vecteurs de support\n",
    "Xs = ??? # FIXME ‚ö†Ô∏è coordonn√©es de vecteurs de support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f8cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des param√®tres de l'hyperplan optimal\n",
    "w_sp = ??? # FIXME ‚ö†Ô∏è vecteur normal √† l'hyperplan\n",
    "b_sp = ??? # FIXME ‚ö†Ô∏è biais de l'hyperplan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e0e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des variables de rel√¢chement\n",
    "Xe_indices = ??? # FIXME ‚ö†Ô∏è indices (bool√©en) des √©chantillons dans la marge ou mal class√©s\n",
    "ye = ??? # FIXME ‚ö†Ô∏è labels de ces √©chantillons erronn√©s\n",
    "Xe = ??? # FIXME ‚ö†Ô∏è coordonn√©es de ces √©chantillons erronn√©s\n",
    "Xi_e = ??? # FIXME ‚ö†Ô∏è variables de rel√¢chement associ√©es √† ces √©chantillons erronn√©s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ea5b02",
   "metadata": {},
   "source": [
    "### Affichage de l'hyperplan optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8526216",
   "metadata": {},
   "source": [
    "Pour finir, on reprend et compl√®te la fonction d'affichage de l'exercice pr√©c√©dent, pour afficher cette fois-ci :\n",
    "- les vecteurs de supports `Xs` et leur classe associ√©e `ys`.\n",
    "- les √©chantillons dans la marge ou mal classifi√©s `Xe` et leur classe associ√©e `ye` (la fonction attend √©galement les variables de rel√¢chement `Xi_e` associ√©es).\n",
    "- l'hyperplan optimal du SVM de param√®tres $\\mathbf{w}^\\star$ et $b^\\star$.\n",
    "- les hyperplans passant par les vecteurs de support des deux classes (d'√©quation ${(\\mathbf{w}^\\star)}^T \\mathbf{x} + b^\\star = \\pm 1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d93f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm(X, y, Xs, ys, Xe, ye, Xi_e, w_star, b_star, label=''):\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.grid(True)\n",
    "    # Classe -1\n",
    "    plt.scatter(X[y==-1,0], X[y==-1,1],\n",
    "                color='red',label='Classe -1',marker='o',edgecolor='k')\n",
    "    plt.scatter(Xs[ys==-1,0], Xs[ys==-1,1],\n",
    "                color='red',label='Vecteurs de support -1',marker='o',edgecolors='k',s=150)\n",
    "    # Classe +1\n",
    "    plt.scatter(X[y==1,0], X[y==1,1],color='blue', label='Classe +1',marker='o',edgecolor='k')\n",
    "    plt.scatter(Xs[ys==1,0], Xs[ys==1,1], \n",
    "                color='blue',label='Vecteurs de support +1',marker='o',edgecolors='k',s=150)\n",
    "    # Points dans la marge (0 < xi < 1)\n",
    "    in_margin = (Xi_e > 0) & (Xi_e < 1)\n",
    "    plt.scatter(Xe[in_margin,0], Xe[in_margin,1], facecolors='none', edgecolors='orange', \n",
    "                    s=150, linewidths=2, label='Dans la marge')\n",
    "    # Points mal class√©s (xi > 1)\n",
    "    misclassified = Xi_e >= 1\n",
    "    plt.scatter(Xe[misclassified,0], Xe[misclassified,1], facecolors='none', edgecolors='purple', \n",
    "                    s=150, linewidths=2, label='Mal class√©')   \n",
    "    # Hyperplan optimal\n",
    "    xmin = X[:,0].min()-0.5\n",
    "    xmax = X[:,0].max()+0.5\n",
    "    x_vals = np.linspace(xmin, xmax, 200)\n",
    "    y_vals = -(w_star[0]*x_vals+b_star)/w_star[1]\n",
    "    plt.plot(x_vals, y_vals, 'k-', label='Hyperplan optimal')\n",
    "    # Hyperplans passant par les vecteurs de support (w.x+b=¬±1)\n",
    "    y_vals_support1 = -(w_star[0]*x_vals+(b_star-1))/w_star[1]\n",
    "    y_vals_support2 = -(w_star[0]*x_vals+(b_star+1))/w_star[1]\n",
    "    plt.plot(x_vals, y_vals_support1, 'k--', label='Hyperplan +1')\n",
    "    plt.plot(x_vals, y_vals_support2, 'k-.', label='Hyperplan -1')\n",
    "    # Titre, labels et l√©gende\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.title('Hyperplan optimal et vecteurs de support ' + label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef3f2b8",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è üöß üë∑  √Ä vous de jouer !\n",
    "\n",
    "Affichez la solution que vous avez trouv√© pour le SVM. Si celle-ci est correcte, la figure devrait faire sens...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45a0fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svm(X,y,Xs,ys,Xe,ye,Xi_e,w_sp,b_sp,label='(scipy)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42179f8d",
   "metadata": {},
   "source": [
    "## R√©solution du SVM avec marge souple en utilisant `cvxopt`\n",
    "\n",
    "Il est maintenant temps de passer √† la r√©solution du SVM √† marge souple gr√¢ce √† la fonction `qp` de `cvxopt`. Le probl√®me dual √©tant toujours un probl√®me QP, vous allez pouvoir appliquer la m√™me d√©marche que pour le SVM √† marge dure.\n",
    "\n",
    "Pour rappel, le format du programme quadratique r√©solu par la fonction `qp` est \n",
    "$$\\begin{aligned}\n",
    "& \\text{minimiser} & \\frac{1}{2} \\mathbf{x}^T \\mathbf{P} \\mathbf{x} + \\mathbf{q}^T \\mathbf{x} \\\\\n",
    "& \\text{sous les contraintes} & \\mathbf{G}\\mathbf{x} \\leq \\mathbf{h} \\\\\n",
    "& & \\mathbf{A}\\mathbf{x} = \\mathbf{b}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Il faut donc sp√©cifier au solveur `qp` les diff√©rentes matrices $\\mathbf{P}, \\mathbf{G}, \\mathbf{A}$ et vecteurs $\\mathbf{q}, \\mathbf{h}, \\mathbf{b}$ au format `matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19da86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxopt\n",
    "from cvxopt import printing\n",
    "cvxopt.matrix_repr = printing.matrix_str_default\n",
    "from cvxopt import matrix\n",
    "from cvxopt.solvers import qp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cee174",
   "metadata": {},
   "source": [
    "La formulation du probl√®me dual du SVM √† marge souple diff√®re de celle du SVM √† marge dure uniquement au niveau des contraintes d'in√©galit√©s $0 \\leq \\alpha_i$, transform√©es en $0 \\leq \\alpha_i \\leq C \\ \\ \\forall i = 1,\\dots,n$. La d√©finition des matrices $\\mathbf{P}$ et $\\mathbf{A}$ et des vecteurs $\\mathbf{q}$ et $\\mathbf{b}$ est donc en tout point similaire √† ce que vous avez fait dans l'[exercice pr√©c√©dent](TP_SVM_exo2.ipynb).\n",
    "\n",
    "En ce qui concerne les contraintes d'in√©galit√© $\\mathbf{G}\\mathbf{x} \\leq \\mathbf{h}$, c'est en revanche un peu plus subtil ici...\n",
    "\n",
    "Pour le SMV √† marge dure, vous avez (normalement) mis les contraintes $0 \\leq \\alpha_i \\ \\ \\forall i = 1,\\dots,n$ sous la forme $\\, -\\alpha_i \\leq 0 \\ \\ \\forall i = 1, \\dots, n$ √©crit sous forme matricielle comme $-\\mathbf{I}_n \\boldsymbol \\alpha \\leq \\mathbf{0}$ avec $\\mathbf{I}_n \\in \\mathbb{R}^{n \\times n}$ la matrice identit√© de taille $n\\times n$ et $\\mathbf{0}$ un vecteur de $\\mathbb{R}^n$ rempli de 0.\n",
    "\n",
    "Pour le SVM √† marge souple en revanche, √©crire les contraintes d'in√©galit√©s $0 \\leq \\alpha_i \\leq C \\ \\ \\forall i = 1,\\dots,n$ sous la forme standard $\\mathbf{G} \\mathbf{x} \\leq \\mathbf{h}$ attendue par `qp` requiert une petite manipulation :\n",
    "\n",
    "* <u>Cas d'une seule contrainte</u> : $0 \\leq \\alpha \\leq C$ <br>\n",
    "Cette contrainte peut se r√©√©crire $ - \\alpha \\leq 0$ et $\\alpha \\leq C$ $\\Leftrightarrow$ $\\left\\{\\begin{align} - \\alpha & \\leq 0 \\\\ \\alpha &\\leq C\\end{align}\\right.$ $\\Leftrightarrow$ $\\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} \\alpha \\leq \\begin{pmatrix} 0 \\\\ C \\end{pmatrix}$ $\\rightarrow$. Dans ce cas, on identifie $\\mathbf{G} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$ et $\\mathbf{h} = \\begin{pmatrix} 0 \\\\ C \\end{pmatrix}$.<br>\n",
    "<br>\n",
    "* <u>Cas de deux contraintes</u> : $0 \\leq \\alpha_1 \\leq C$ et $0 \\leq \\alpha_2 \\leq C$<br>\n",
    "En appliquant la m√™me id√©e, on a $\\left\\{\\begin{align} - \\alpha_1 & \\leq 0 \\\\ - \\alpha_2 & \\leq 0 \\\\ \\alpha_1 &\\leq C \\\\ \\alpha_2 & \\leq C\\end{align}\\right.$ $\\ \\Leftrightarrow \\ $ $\\begin{pmatrix} -1  & 0 \\\\ 0 & -1 \\\\ 1 & 0 \\\\ 0 & 1\\end{pmatrix} \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\end{pmatrix} \\leq \\begin{pmatrix} 0 \\\\ 0 \\\\ C \\\\ C\\end{pmatrix}$, ce qui permet l√† encore d'identifier la matrice $\\mathbf{G}$ et le vecteur $\\mathbf{h}$ attendus par `qp`.<br><br>\n",
    "* <u> Cas de $n$ contraintes</u> : $0 \\leq \\alpha_i \\leq C \\ \\ \\forall i=1,\\dots,n$<br>\n",
    "√Ä vous de g√©n√©raliser la relation pr√©c√©dente au cas de $n$ contraintes..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e3e533",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è üöß üë∑  √Ä vous de jouer !\n",
    "\n",
    "R√©solvez le probl√®me dual du SVM √† marge souple gr√¢ce au solveur `qp` de `cvxopt` en utilisant la m√™me d√©marche que le SVM √† marge dure (rappel√©e ici) :\n",
    "1. La reformulation du probl√®me dual $(D)$ comme un probl√®me de **minimisation** pour pouvoir appliquer `minimize` reste valable dans le cas de `qp`, qui attend √©galement de **minimiser** une fonction quadratique donn√©e.\n",
    "2. Identifiez les matrices $\\mathbf{P}$, $\\mathbf{G}$ et $\\mathbf{A}$ et les vecteurs $\\mathbf{q}$, $\\mathbf{h}$, $\\mathbf{b}$ de la forme standard attendue par `qp` √† partir de la formulation du probl√®me dual, et en utilisant la manipulation pr√©sent√©e ci-dessus pour $\\mathbf{G}$ et $\\mathbf{h}$) et d√©finissez les au format `matrix` de `cvxopt`.<br>\n",
    "3. R√©solvez num√©riquement le probl√®me dual gr√¢ce √† `qp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1653114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d√©finition des matrices du probl√®me QP\n",
    "C = ??? # FIXME ‚ö†Ô∏è Hyperparam√®tre C\n",
    "P = ??? # FIXME ‚ö†Ô∏è\n",
    "q = ??? # FIXME ‚ö†Ô∏è\n",
    "G = ??? # FIXME ‚ö†Ô∏è\n",
    "h = ??? # FIXME ‚ö†Ô∏è\n",
    "A = ??? # FIXME ‚ö†Ô∏è\n",
    "b = ??? # FIXME ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744bc7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©solution du probl√®me dual\n",
    "result_cvxopt = qp(???) # FIXME ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5c14d6",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è üöß üë∑  √Ä vous de jouer !\n",
    "\n",
    "Une fois le probl√®me dual r√©solu par `qp` et le multiplicateur de Lagrange optimal $\\boldsymbol \\alpha^\\star$ obtenu, d√©terminez :\n",
    "1. Le vecteur normal $\\mathbf{w}^\\star$ √† l'hyperplan optimal : $\\displaystyle \\mathbf{w}^\\star = \\sum_{i=1}^{n} \\alpha_i^\\star y_i \\mathbf{x}_i$\n",
    "2. Les vecteurs de support $\\mathbf{x}_s$ (tels que $0 < \\alpha_s^\\star < C$).\n",
    "3. Le biais $ b^\\star$ de l'hyperplan optimal $y_i \\big( (\\mathbf{w}^\\star)^T \\mathbf{x}_i + b^\\star)\\big) = 1$ calcul√© √† partir d'un vecteur de support. \n",
    "4. Les √©chantillons $\\mathbf{x}_i$ √† l'int√©rieur de la marge ou mal class√©s (tels que $\\alpha_i^\\star = C $) et leur variable de rel√¢chement associ√©e $\\xi_i^\\star = 1 - y_i\\big((\\mathbf{w}^{\\star})^T \\mathbf{x}_i + b^\\star \\big)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d2da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.array(result_cvxopt['x']).flatten()\n",
    "\n",
    "# Calcul des vecteurs de support\n",
    "Xs_indices = ??? # FIXME ‚ö†Ô∏è indices (bool√©en) des vecteurs de support\n",
    "ys = ??? # FIXME ‚ö†Ô∏è labels des vecteurs de support\n",
    "Xs = ??? # FIXME ‚ö†Ô∏è coordonn√©es de vecteurs de support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7783d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des param√®tres de l'hyperplan optimal\n",
    "w_cvxopt = ??? # FIXME ‚ö†Ô∏è vecteur normal √† l'hyperplan\n",
    "b_cvxopt = ??? # FIXME ‚ö†Ô∏è biais de l'hyperplan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223c80e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des variables de rel√¢chement\n",
    "Xe_indices = ??? # FIXME ‚ö†Ô∏è indices (bool√©en) des √©chantillons dans la marge ou mal class√©s\n",
    "ye = ??? # FIXME ‚ö†Ô∏è labels de ces √©chantillons erronn√©s\n",
    "Xe = ??? # FIXME ‚ö†Ô∏è coordonn√©es de ces √©chantillons erronn√©s\n",
    "Xi_e = ??? # FIXME ‚ö†Ô∏è variables de rel√¢chement associ√©es √† ces √©chantillons erronn√©s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7a71f5",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è üöß üë∑  √Ä vous de jouer !\n",
    "\n",
    "Pour finir, affichez la solution que vous avez trouv√© pour le SVM avec le dual r√©solu par `qp`. Si celle-ci est correcte, la figure devrait donc √™tre identique √† celle obtenue pour `minimize` (sous r√©serve que celle-ci aussi soit correcte bien sur...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1bab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svm(X,y,Xs,ys,Xe,ye,Xi_e,w_cvxopt,b_cvxopt,label='(cvxopt)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b424aa1",
   "metadata": {},
   "source": [
    "## Un peu de lecture üìñ : r√©solution du SVM √† marge souple en utilisant `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b546429",
   "metadata": {},
   "source": [
    "Quand on parle de SVM (donc d'un probl√®me de classification), on pense en g√©n√©ral √† `scikit-learn`. Votre biblioth√®que favorite de _machine learning_ impl√©mente √©videmment le SVM. Il serait logique que le r√©sultat de `scikit-learn` soit identique √† ceux de `minimize` et `qp`.\n",
    "\n",
    "L'API de `scikit-learn` est bien diff√©rente de celles de `minimize` et `qp` puisqu'elle se focalise sur les aspects de _machine learning_ (en m√™me temps, c'est ce qu'on attend d'elle) plut√¥t que les aspects d'optimisation. On peut malgr√© tout s'en sortir en fouillant un peu les diff√©rents attributs du mod√®le de SVM impl√©ment√© dans `scikit-learn` : c'est ce qu'il vous est propos√© de lire dans cette partie (votre prof a fait le boulot √† votre place üòâ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2edd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e7e03",
   "metadata": {},
   "source": [
    "D√©finition du mod√®le de SVM lin√©aire (_aka_ pas la version noyau ici) avec l'hyperparam√®tre $C$ d√©fini √† la m√™me valeur que pour `minimize` et `qp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10dde11",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = SVC(kernel='linear',C=C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b201b2b",
   "metadata": {},
   "source": [
    "Entra√Ænement du SVM sur l'int√©gralit√© du jeu de donn√©es (pas besoin de train/test split ici puisqu'on ne va pas tester les performances en pr√©diction du mod√®le)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d3fb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c806ceb6",
   "metadata": {},
   "source": [
    "Une fois l'entra√Ænement r√©alis√©, on peut r√©cup√©rer le vecteur normal √† l'hyperplan optimal gr√¢ce √† l'attribut `coef_` et le biais de l'hyperplan gr√¢ce √† l'attribut `intercept_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed61f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vecteur normal √† l'hyperplan optimal :\",SVM.coef_.flatten()) # flatten() pour avoir une shape (2,)\n",
    "print(\"Biais de l'hyperplan optimal :\",SVM.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e4a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_sk = SVM.coef_.flatten()\n",
    "b_sk = SVM.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75a15af",
   "metadata": {},
   "source": [
    "L'API de `scikit-learn` ne permet pas d'acc√©der aux multiplicateurs de Lagrange optimaux $\\alpha_i^\\star$ (puisqu'encore une fois, ils n'ont pas d'int√©r√™t en soit pour des op√©rations pratiques de _machine learning_). Elle permet en revanche d'acc√©der aux indices des vecteurs de support par l'attribut `support_` et √† leurs coordonn√©es `support_vectors_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfbdca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Indice des vecteurs de support :\",SVM.support_)\n",
    "print(\"Coordonn√©es des vecteurs de support :\\n\",SVM.support_vectors_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883b897f",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Pour `scikit-learn`, les vecteurs de support ne sont pas uniquement les √©chantillons qui v√©rifient $y_i(\\mathbf{w}^T \\mathbf{x}_i + b) = 1$ (donc sur les hyperplans $\\pm 1$), mais √©galement ceux qui tombent dans la marge ou du mauvais c√¥t√© de l'hyperplan optimal : en bref, pour `scikit-learn`, les vecteurs de support sont ceux dont le multiplicateur de Lagrange associ√© est strictement positif $\\alpha_i^\\star > 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775ba6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_indices_all = SVM.support_ # indice de tous les vecteurs de support pour scikit-learn\n",
    "ys_all = y[Xs_indices_all] # labels de ces vecteurs de support\n",
    "Xs_all = X[Xs_indices_all,:] # coordonn√©es de ces vecteurs de support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b277e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.grid(True)\n",
    "# Classe -1\n",
    "plt.scatter(Xs_all[ys_all==-1,0], Xs_all[ys_all==-1,1],\n",
    "            color='red',label='Classe -1',marker='o',edgecolor='k')\n",
    "# Classe +1\n",
    "plt.scatter(Xs_all[ys_all==1,0], Xs_all[ys_all==1,1],\n",
    "            color='blue', label='Classe +1',marker='o',edgecolor='k')\n",
    "# Hyperplan optimal\n",
    "xmin = Xs_all[:,0].min()-0.25\n",
    "xmax = Xs_all[:,0].max()+0.25\n",
    "x_vals = np.linspace(xmin, xmax, 200)\n",
    "y_vals = -(w_sk[0]*x_vals+b_sk)/w_sk[1]\n",
    "plt.plot(x_vals, y_vals, 'k-', label='Hyperplan optimal')\n",
    "# Hyperplans passant par les vecteurs de support (w.x+b=¬±1)\n",
    "y_vals_support1 = -(w_sk[0]*x_vals+(b_sk-1))/w_sk[1]\n",
    "y_vals_support2 = -(w_sk[0]*x_vals+(b_sk+1))/w_sk[1]\n",
    "plt.plot(x_vals, y_vals_support1, 'k--', label='Hyperplan +1')\n",
    "plt.plot(x_vals, y_vals_support2, 'k-.', label='Hyperplan -1')\n",
    "# Titre, labels et l√©gende\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('Hyperplan optimal et vecteurs de support de scikit-learn')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f4e6cb",
   "metadata": {},
   "source": [
    "Pour identifier les \"vrais\" vecteurs de support au sens o√π on l'entend depuis le d√©but, √† savoir ceux dont le multiplicateur de Lagrange optimal est $0 < \\alpha_i^\\star < C$, il suffit donc de chercher parmis ces points ceux qui v√©rifient l'√©quation $y_i(\\mathbf{w}^T \\mathbf{x}_i + b) = 1$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edba553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_all*(np.dot(Xs_all,w_sk) + b_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b101e21c",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è La pr√©cision num√©rique du calcul pr√©c√©dent est telle qu'un seuil dur `== 1` est vou√© √† l'√©chec. En revanche, une tol√©rance de $10^{-3}$ fera l'affaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffe852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_indices = np.isclose(ys_all*(np.dot(Xs_all,w_sk) + b_sk),1,atol=1e-3)\n",
    "ys = ys_all[Xs_indices]\n",
    "Xs = Xs_all[Xs_indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9310039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coordonn√©es des 'vrais' vecteurs de support :\\n\",Xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c769332",
   "metadata": {},
   "source": [
    "Ne reste qu'√† r√©cup√©rer les coordonn√©es `Xe` des vecteurs dans la marge ou mal class√©s, leur label `ye` ainsi que la variable de rel√¢chement `Xi_e` qui leur est associ√©e.\n",
    "\n",
    "Cette derni√®re se calcule via la relation $\\xi_i^\\star = 1 - y_i\\big((\\mathbf{w}^{\\star})^T \\mathbf{x}_i + b^\\star \\big)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9b575",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xe = Xs_all[~Xs_indices,:]\n",
    "ye = ys_all[~Xs_indices]\n",
    "Xi_e = 1 - ye*(np.dot(Xe, w_sk)+b_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4048ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variables de rel√¢chement pour les √©chantillons dans la marge / mal class√©s :\\n\",Xi_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b2725f",
   "metadata": {},
   "source": [
    "On peut d'ailleurs v√©rifier num√©riquement que ces variables de rel√¢chement $\\xi_i^\\star$ sont toutes strictement positives (encore heureux puisque c'√©tait l'une des contraintes du probl√®me primal...).<br>\n",
    "Les √©chantillons pour lesquels $0 < \\xi_i^\\star < 1$ sont dans la marge mais du bon c√¥t√© de l'hyperplan s√©parateur. Ceux pour lesquels $\\xi_i^\\star > 1$ sont du mauvais c√¥t√© de l'hyperplan s√©parateur.\n",
    "\n",
    "\n",
    "Pour finir, on peut afficher le scatterplot des √©chantillons avec la solution du SVM √† marge souple calcul√©e par `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d482cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svm(X,y,Xs,ys,Xe,ye,Xi_e,w_sk,b_sk,label='(scikit-learn)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae1617c",
   "metadata": {},
   "source": [
    "Cette derni√®re concorde bien visuellement avec les solution trouv√©es par `minimize` et `qp` (√©videmment sous r√©serve que ce que vous avez fait est correct...)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7fd8e0",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è üöß üë∑  √Ä vous de jouer !\n",
    "\n",
    "Pour finir, v√©rifiez que les param√®tres $\\mathbf{w}^\\star$ et $b^\\star$ trouv√©s par `minimize`, `qp` et `scikit-learn` concordent bien num√©riquement : \n",
    "- est-ce vraiment le cas ?\n",
    "- que pouvez vous en conclure ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca4b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_sp ??? w_cvxopt # FIXME ‚ö†Ô∏è est-ce que les vecteurs normaux aux hyperplans concordent ?\n",
    "w_sp ??? w_sk # FIXME ‚ö†Ô∏è\n",
    "w_sk ??? w_cvxopt # FIXME ‚ö†Ô∏è\n",
    "b_sp ??? b_cvxopt # FIXME ‚ö†Ô∏è est-ce que les biais des hyperplans concodent ?\n",
    "b_sp ??? b_sk # FIXME ‚ö†Ô∏è\n",
    "b_sk ??? b_cvxopt # FIXME ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a70ed",
   "metadata": {},
   "source": [
    "On peut donc en conclude que ‚ö†Ô∏è FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157fc8ef",
   "metadata": {},
   "source": [
    "## BENCH BENCH BENCH !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45535b3b",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è üöß üë∑  √Ä vous de jouer !\n",
    "\n",
    "Il ne vous aura sans doute pas √©chapp√© que dans cet exercice, vous avez r√©solu le SVM √† marge souple pour une seule configuration donn√©e d'√©chantillons \"presque lin√©airement s√©parables\", et pour une seule valeur de l'hyperparam√®tre $C=1$.\n",
    "\n",
    "S'il vous reste encore du temps (et de l'√©nergie), vous pouvez maintenant benchmarker (comme √† vos plus belles heures d'OCVX1) le comportement du SVM √† marge souple en faisant varier la s√©parabilit√© du jeu de donn√©es (en modifiant la valeur de `classe_sep` et de `scale`) et/ou la valeur de l'hyperparam√®tre $C$ pour voir leur influence sur les crit√®res de votre choix, comme par exemple :\n",
    "- la valeur de la marge normalis√©e $\\frac{2}{\\| \\mathbf{w} \\|}$\n",
    "- la valeur de la p√©nalisation $\\displaystyle \\sum_{i= 1}^n \\xi_i$ de la fonction objective\n",
    "- la valeur de la fonction objective elle m√™me $\\frac{1}{2} \\| \\mathbf{w} \\|^2 + C \\sum_{i=1}^{n} \\xi_i$\n",
    "- le nombre d'√©chantillons tombant dans la marge ou mal classifi√©s\n",
    "- le nombre d'it√©rations n√©cessaires au solveur pour converger vers la solution\n",
    "- etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6a03d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME ‚ö†Ô∏è BENCH BENCH BENCH !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1b89b8",
   "metadata": {},
   "source": [
    "# Bravo ! üëèüçæ\n",
    "\n",
    "Et b√© ! Vous en √™tes arriv√©s au bout, c'√©tait quand m√™me un sacr√© morceau ce TP ! Bravo √† vous üçª"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
