{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "worthy-bailey",
   "metadata": {},
   "source": [
    "EPITA 2023 IML lab02_clustering_04-EM-GMM v2023-03-27_103407 by G. Tochon & J. Chazalon\n",
    "\n",
    "<div style=\"overflow: auto; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"Creative Commons License\" src='img/CC-BY-4.0.png' style='float: left; margin-right: 20px'>\n",
    "    \n",
    "This work is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-tennis",
   "metadata": {},
   "source": [
    "# Lab 2, part 4: Reimplement EM to train a GMM\n",
    "\n",
    "![](img/1280px-ClusterAnalysis_Mouse.svg.png)\n",
    "*Guess why it is this called the \"mouse\" dataset…*\n",
    "\n",
    "\n",
    "In this first part, we will focus on the famous **[GMMs](https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model)** which are estimated using the equally famous [Expectation-Maximization (EM) algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm).\n",
    "\n",
    "Compared to K-Means, GMMs are a bit more complex to fit but they provide a much richer model of the underlying data distribution.\n",
    "Indeed, GMMs represent a data distribution as a set of modes, and for each mode we estimate:\n",
    "\n",
    "- its mean ($\\mu$), alike cluster centroïds for K-Means;\n",
    "- its variance ($\\Sigma$) in every direction;\n",
    "- its amplitude  ($\\pi$).\n",
    "\n",
    "This allows to models points clouds with anisotropic spreads, and also to perform soft assignments (i.e. compute an assignment to multiple clusters for each point).\n",
    "\n",
    "Needless to say, the complexity of parameter estimation is highly correlated with the richness of the representation.\n",
    "\n",
    "Please note that Gaussian Mixture Models are a particular case of Mixture models, and many variants can complexify our representation as per our needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-remedy",
   "metadata": {},
   "source": [
    "Here, we will focus on re-implementing a simple EM algorithm to fit a simplied GMM.\n",
    "Our code will be very unefficient, and inaccurate (in particular because we do not compute values in the log-probability space).\n",
    "\n",
    "We will then conduct the same kind of study as for the K-Means to understand how such model can be used:\n",
    "\n",
    "- play with scikit-learn's implementation\n",
    "- understand the role of the covariance matrix\n",
    "- understand how to select the number of components\n",
    "- re-implement an EM algorithm to fit a GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-syria",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-delta",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-singing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-sculpture",
   "metadata": {},
   "source": [
    "## Some sample data\n",
    "We suggest you work with the following set of samples points to get started.\n",
    "It will enable us to illustrated in a simple way the key messages we want to share with you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_centers = np.array(\n",
    "    [[ 0.2,  2.3],\n",
    "     [-1.5 ,  2.3],\n",
    "     [-2.8,  1.8],\n",
    "     [-2.8,  2.8],\n",
    "     [-2.8,  1.3]])\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n",
    "X, y = make_blobs(n_samples=2000, centers=blob_centers,\n",
    "                  cluster_std=blob_std, random_state=7)\n",
    "transformation = [[0.6, -0.7], [-0.2, 0.9]]\n",
    "X_aniso = np.dot(X, transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(X, y=None, /, size=1, label=None):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=size, label=label)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-rendering",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "plt.subplot(1,2,1)\n",
    "plot_clusters(X_aniso,y)\n",
    "plt.title(\"Our sample data with cluster assignment\")\n",
    "plt.axis(\"equal\")\n",
    "plt.subplot(1,2,2)\n",
    "plot_clusters(X_aniso)\n",
    "plt.title(\"Our sample data as our self-supervised algorithm will see it\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-symposium",
   "metadata": {},
   "source": [
    "## 1. Using scikit-learn's GMM\n",
    "We will first try to use and understand [scikit-learn's implementation of GMM](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html).\n",
    "\n",
    "In the next section, we will try to propose our own implementation for the same API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-fitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "    \n",
    "    # Draw the Ellipse\n",
    "    for nsig in range(1, 4):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
    "                             angle, **kwargs))\n",
    "        \n",
    "def plot_gmm(gmm, X, label=True, ax=None):\n",
    "    '''\n",
    "    gmm: GMM model with sklearn-compatible API\n",
    "    X: data to model (np.array, 2D (samples, features))\n",
    "    '''\n",
    "    ax = ax or plt.gca()\n",
    "    labels = gmm.fit(X).predict(X)\n",
    "    if label:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=10, cmap='viridis', zorder=2, alpha=0.5)\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2, alpha=0.5)\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    p = gmm.get_params()\n",
    "    w_factor = 0.2 / gmm.weights_.max()\n",
    "    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
    "        draw_ellipse(pos, covar, alpha=w * w_factor)\n",
    "    plt.title(f\"GMM K: {p['n_components']};  iter: {gmm.n_iter_}; cov_type: {p['covariance_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-holocaust",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "<b>Using the previous `plot_gmm()` function, plot the resulting density estimation obtained with different parameters for `GaussianMixture` model.</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME\n",
    "# gmm = ???\n",
    "# ...\n",
    "# plot_gmm(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=5, random_state=2, covariance_type=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plot_gmm(gmm, X_aniso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-alabama",
   "metadata": {},
   "source": [
    "## 2. Reimplement our own GMM\n",
    "This is a pretty tough part, we will try to focus on reimplementing the math properly.\n",
    "\n",
    "Under the hood, a Gaussian mixture model is very similar to k-means: it uses an expectation–maximization approach which qualitatively does the following:\n",
    "\n",
    "1. Choose starting guesses for the location and shape\n",
    "\n",
    "2. Repeat until converged:\n",
    "   - E-step: for each point, find weights encoding the probability of membership in each cluster\n",
    "   - M-step: for each cluster, update its location, normalization, and shape based on all data points, making use of the weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-cable",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "<b>Reimplement you GMM training using EM. We suggest that you follow the advisory instructions below.</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-interface",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "polish-budget",
   "metadata": {},
   "source": [
    "### A. Implement a Gaussian function\n",
    "The goal is to model the generation of some $\\mathbf{X}\\ \\sim\\ \\mathcal{N}(\\boldsymbol\\mu,\\, \\boldsymbol\\Sigma)$, i.e. of an observation randomly drawn from a normal distribution of mean $\\boldsymbol\\mu$ and variance $\\boldsymbol\\Sigma$.\n",
    "\n",
    "In this case, if we note\n",
    "\\begin{equation}\n",
    "{\\mathbf x} = (x_1,\\ldots,x_n)\n",
    "\\end{equation}\n",
    "our multivariate observation, then \n",
    "according to Wikipedia's page on [Multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Non-degenerate_case) \n",
    "the multivariate normal distribution is said to be \"non-degenerate\" when the symmetric covariance matrix $\\boldsymbol {\\Sigma }$ is positive definite (**our case, no negative values**).\n",
    "\n",
    "In this case the distribution has density:\n",
    "\\begin{equation}\n",
    "\\large\n",
    "p(\\mathbf x | \\mathbf\\mu, \\mathbf\\Sigma) = \\frac 1 {\\sqrt{({2\\pi})^n|\\Sigma|}} \\exp\\left(-\\frac 1 2 ({\\mathbf x}-{\\boldsymbol\\mu})^\\mathrm{T}{\\boldsymbol\\Sigma}^{-1}({\\mathbf x}-{\\boldsymbol\\mu})\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf {x}$ is a real n-dimensional column vector and $|{\\boldsymbol {\\Sigma }}|\\equiv \\det {\\boldsymbol {\\Sigma }}$ is the determinant of ${\\boldsymbol {\\Sigma }}$.\n",
    "The quantity ${\\sqrt {({\\mathbf {x} }-{\\boldsymbol {\\mu }})^{\\mathrm {T} }{\\boldsymbol {\\Sigma }}^{-1}({\\mathbf {x} }-{\\boldsymbol {\\mu }})}}$ is known as the Mahalanobis distance, which represents the distance of the test point $\\mathbf {x}$ from the mean $\\boldsymbol {\\mu }$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-patrol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement this \"basic\" function\n",
    "# HINTS use np.pi, np.linalg.det, array.T, np.dot, np.linalg.inv\n",
    "\n",
    "def gaussian_1(x, mu, sigma):\n",
    "    '''x: vector\n",
    "    returns the scalar value of the gaussian (mu, sigma) at x.'''\n",
    "    # ???\n",
    "    pass\n",
    "\n",
    "\n",
    "def gaussian_n(X, mu, sigma):\n",
    "    '''X: matrix of row vectors\n",
    "    returns the scalar value of the gaussian (mu, sigma) for each point in X.'''\n",
    "    # print(X.shape, mu.shape, sigma.shape)\n",
    "    G = np.apply_along_axis(lambda x: gaussian_1(x, mu, sigma), -1, X)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some tests to check your implementation\n",
    "x0 = np.array([[0.05, 1.413, 0.212], [0.85, -0.3, 1.11], [11.1, 0.4, 1.5], [0.27, 0.12, 1.44], [88, 12.33, 1.44]])\n",
    "mu = np.mean(x0, axis=0)\n",
    "cov = np.dot((x0 - mu).T, x0 - mu) / (x0.shape[0] - 1)\n",
    "\n",
    "Y = gaussian_n(x0, mu, cov)\n",
    "Y\n",
    "# should return\n",
    "# array([0.00159853, 0.00481869, 0.00276259, 0.0014309 , 0.00143998])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-daily",
   "metadata": {},
   "source": [
    "### B. Initialization\n",
    "\n",
    "To initialize our GMM, we must initialise our parameters $\\pi_k$, $\\mu_k$, and $\\Sigma_k$. In this case, we are going to use the results of KMeans as an initial value for $\\mu_k$, set $\\pi_k$ to one over the number of clusters and $\\Sigma_k$ to the identity matrix. We could also use random numbers for everything, but using a sensible initialisation procedure will help the algorithm achieve better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO complete the code of the \"_initialize()\" method in the class below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-porcelain",
   "metadata": {},
   "source": [
    "### C. Expectation step\n",
    "This is actually the easiest part.\n",
    "\n",
    "\n",
    "We should now calculate $\\gamma(z_{nk})$. We can achieve this by means of the following expression:\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "\\gamma{(z_{nk})}=\\frac {\\pi_k\\mathcal N(\\mathbf x_n| \\mathbf\\mu_k, \\mathbf\\Sigma_k)}{\\sum_{j=1}^K\\pi_j\\mathcal N(\\mathbf x_n| \\mathbf\\mu_j, \\mathbf\\Sigma_j)}\n",
    "\\end{equation}\n",
    "\n",
    "Which is the probability of the $k$-th cluster emitting the observation $x$.\n",
    "\n",
    "We need to compute the value for all samples, for all clusters.\n",
    "\n",
    "**This is equivalent to predicting the weighted probability for each observations, normalized by the sum of all predictions for each single observation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO complete the code of the \"predict_proba()\" method in the class below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-deviation",
   "metadata": {},
   "source": [
    "### D. Maximization step\n",
    "\n",
    "Let us now implement the maximization step. Since $\\gamma(z_{nk})$ is common to the expressions for $\\pi_k$, $\\mu_k$ and $\\Sigma_k$, we can simply define:\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "N_k=\\sum_{n=1}^N\\gamma({z_{nk}})\n",
    "\\end{equation}\n",
    "\n",
    "(Note hat $N_k$ is a vector of $k$ components.)\n",
    "\n",
    "And then we can calculate the revised parameters by using (closed-form solution to the minimization of the log-likelihood, see point E.):\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "\\pi_k^*=\\frac {N_k} N\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "\\mu_k^*=\\frac 1 {N_k} \\sum_{n=1}^N\\gamma({z_{nk}})\\mathbf x_n\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "\\Sigma_k^*=\\frac 1 {N_k} \\sum_{n=1}^N\\gamma({z_{nk}})(\\mathbf x_n-\\mathbf\\mu_k)(\\mathbf x_n-\\mathbf\\mu_k)^T\n",
    "\\end{equation}\n",
    "\n",
    "Note: To calculate the covariance, we define an auxiliary variable __diff__ that contains $(x_n-\\mu_k)^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO complete the code of the \"_maximization_step()\" method in the class below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-camping",
   "metadata": {},
   "source": [
    "### E. Convergence and Log-likelihood\n",
    "\n",
    "Let us now determine the [log-likelihood](https://en.wikipedia.org/wiki/Likelihood_function#Log-likelihood) of the model. It is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "\\ln p(\\mathbf X)=\\sum_{n=1}^N\\ln\\sum_{k=1}^K\\pi_k\\mathcal N(\\mathbf x_n|\\mu_k,\\Sigma_k)\n",
    "\\end{equation}\n",
    "\n",
    "The second summation has already been calculated in the __expectation_step__ function. Let is just make use of it.\n",
    "\n",
    "Convergence is reached when the difference between the log-likelihood of two consecutive steps decreases below a threshold tolerance value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO complete the code of the \"get_likelihood()\" method in the class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans  # Needed for initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGMM:\n",
    "    def __init__(self, n_components, *, random_state=None, max_iter=300, tol=0.0001):\n",
    "        self.n_components = n_components\n",
    "        self._random_state = random_state\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        \n",
    "        # means_array-like of shape (n_components, n_features): The mean of each mixture component.\n",
    "        self.means_ = None\n",
    "        # covariances_array-like: The covariance of each mixture component.\n",
    "        # shape is (n_components, n_features, n_features) if covariance type is 'full'\n",
    "        self.covariances_ = None\n",
    "        # weights_: array-like of shape (n_components,): The weights of each mixture components.\n",
    "        self.weights_ = None\n",
    "\n",
    "    def _print(self):\n",
    "        print(\"=\"*20)\n",
    "        print(\"Weights\")\n",
    "        print(self.weights_)\n",
    "        print(\"Means\")\n",
    "        print(self.means_)\n",
    "        print(\"Covars\")\n",
    "        print(self.covariances_)\n",
    "\n",
    "    def fit(self, X):\n",
    "        self._initialize(X)\n",
    "        self.n_iter_ = 0\n",
    "        prev_llh = np.infty\n",
    "        llh_change = np.infty\n",
    "        while self.n_iter_ < self.max_iter and llh_change > self.tol:\n",
    "            print(f\"ITER {self.n_iter_}\")\n",
    "            # self._print()\n",
    "            # Estimation step -----\n",
    "            gamma_nk, gamma_n = self._expectation_step(X)\n",
    "\n",
    "            # Maximization step -----\n",
    "            new_weights, new_means, new_covariances = self._maximization_step(X, gamma_nk)\n",
    "\n",
    "            # Likelihood -----\n",
    "            llh, _ = self.get_likelihood(X, gamma_n)\n",
    "            print(f\"log likelihood: {llh:0.4f}\")\n",
    "\n",
    "            # Update parameters -----\n",
    "            self.weights_ = new_weights\n",
    "            self.means_ = new_means\n",
    "            self.covariances_ = new_covariances\n",
    "\n",
    "            # Update indicators -----\n",
    "            llh_change = np.abs(prev_llh - llh)\n",
    "            prev_llh = llh\n",
    "            self.n_iter_ += 1\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Simply pick the best Gaussian for each observation\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.atleast_2d(X)\n",
    "        if self.means_ is None:\n",
    "            raise RuntimeError(\"You must call fit() before you can predict().\")\n",
    "        probas = np.zeros((X.shape[0], self.n_components), dtype=np.float32)\n",
    "        for k in range(self.n_components):\n",
    "            pi_k = self.weights_[k]\n",
    "            mu_k = self.means_[k]\n",
    "            sigma_k = self.covariances_[k]\n",
    "            # FIXME #############################################\n",
    "            # probas[:,k] = ???\n",
    "        return probas\n",
    "    \n",
    "\n",
    "    def _initialize(self, X):\n",
    "        num_type = np.float32\n",
    "        n_features = X.shape[1]\n",
    "        # FIXME #############################################\n",
    "        # Use kmeans for first guess. IRL there may be smarter ways!\n",
    "        # ...\n",
    "        # (shape: (n_components, n_features))\n",
    "        # self.means_ = ???\n",
    "        # and now, how do we init weights and covariances?\n",
    "        # We pick the simplest possible choice:\n",
    "        # weights are equals and sum to 1\n",
    "        # (shape: (n_components, ) )\n",
    "        # self.weights_ = ???\n",
    "        # covariance is unit spherical (i.e. Identity(self.n_components))\n",
    "        # (shape: (n_components, n_features, n_features) )\n",
    "        # ???\n",
    "        # self.covariances_ = ???\n",
    "        # ???\n",
    "    \n",
    "    \n",
    "    def _expectation_step(self, X):\n",
    "        '''X: shape:(n_samples, n_features) all observations'''\n",
    "        probas = self.predict_proba(X)\n",
    "        # normalize by sum of probas for every cluster for each x_i\n",
    "        denom = np.sum(probas, axis=1)\n",
    "        probas /= denom.reshape((-1,1))\n",
    "        return probas, denom  # shape:(n_samples, n_components) and (n_samples,)\n",
    "\n",
    "    def _maximization_step(self, X, gamma_nk):\n",
    "        assert X.shape[0] == gamma_nk.shape[0]\n",
    "        N = gamma_nk.shape[0]\n",
    "        N_k = np.sum(gamma_nk, axis=0)\n",
    "\n",
    "        # Compute new weights\n",
    "        # (share of weights for samples \"voting\" for each cluster)\n",
    "        # FIXME #############################################\n",
    "        # new_weights = ???\n",
    "\n",
    "        # Compute new means\n",
    "        # (barycenter of samples weighted by their membership to each cluster)\n",
    "        # FIXME #############################################\n",
    "        # new_means = ???\n",
    "\n",
    "        # Compute new covariances\n",
    "        # (use new_means! covar of samples weighted by their membership to each cluster)\n",
    "        # FIXME #############################################\n",
    "        # new_covariances = ???\n",
    "\n",
    "        return new_weights, new_means, new_covariances\n",
    "\n",
    "\n",
    "    def get_likelihood(self, X, gamma_n):\n",
    "        # FIXME #############################################\n",
    "        # sample_likelihoods = ???\n",
    "        return np.sum(sample_likelihoods), sample_likelihoods\n",
    "    \n",
    "    def get_params(self):\n",
    "        return {\n",
    "            'n_components': self.n_components, \n",
    "            'covariance_type': 'full'\n",
    "            }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-winning",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fossil-ottawa",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "<b>Try your GMM implementation on our data, for a couple of cluster values. This will be slow!</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plot_gmm(MyGMM(n_components=5, random_state=2), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-rebound",
   "metadata": {},
   "source": [
    "Some interesting questions:\n",
    "\n",
    "- Can we initialize better?\n",
    "- Do we need full covariance? How would the computations be impacted by stronger assumptions on the covariances?\n",
    "- Can we compare the value of the log-likelihood between different trainings? problems? for different numbers of components?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-aviation",
   "metadata": {},
   "source": [
    "*Hey, let's take some notes here!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-budapest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-thousand",
   "metadata": {},
   "source": [
    "## 3. Select the number of components\n",
    "There is not no regularization in the algorithm, just bias/capacity tuning based on the number of components.\n",
    "There are several criterions to select the number of components, always based on how well our model predict the **training** data.\n",
    "\n",
    "Some criterions:\n",
    "\n",
    "- [Bayesian information criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion)\n",
    "- [Akaike information criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion)\n",
    "\n",
    "\n",
    "### BIC\n",
    "The BIC is formally defined as\n",
    "\n",
    "$\\mathrm {BIC} =k\\ln(n)-2\\ln({\\hat {L}})$\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "- ${\\hat {L}}$ = the maximized value of the likelihood function of the model $M$, i.e. ${\\hat {L}}=p(x\\mid {\\widehat {\\theta }},M)$, where ${\\widehat {\\theta }}$ are the parameter values that maximize the likelihood function;\n",
    "- $x$ = the observed data;\n",
    "- $n$ = the number of data points in $x$, the number of observations, or equivalently, the sample size;\n",
    "- $k$ = the number of parameters estimated by the model\n",
    "\n",
    "\n",
    "### AIC\n",
    "Let $k$ be the number of estimated parameters in the model. Let ${\\hat {L}}$ be the maximum value of the likelihood function for the model. Then the AIC value of the model is the following. \n",
    "\n",
    "$\\mathrm {AIC} \\,=\\,2k-2\\ln({\\hat {L}})$\n",
    "\n",
    "### What is $\\ln({\\hat {L}})$?\n",
    "It is the mean log-probability of each of the samples as predicted by our trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-violin",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "<b>You can compute the BIC and the AIC directly from a `GaussianMixture` object from scikit-learn. Try to plot the values for these criterions for a range of Gaussians (number of clusters).</b>\n",
    "\n",
    "*Hint:* you can reuse the code from https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_selection.html\n",
    "    \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-playlist",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "<b>You can try to recompute these criterions on your own GMM class.</b>\n",
    "\n",
    "*Hint:* check scikit-learn's code!\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-guess",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-edition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "miniature-arcade",
   "metadata": {},
   "source": [
    "# Job done!\n",
    "Wow, that was a tough one! Congratulations to you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-recall",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
